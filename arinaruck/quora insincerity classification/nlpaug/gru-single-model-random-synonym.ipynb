{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/18/ef5215832f523c29f6e0c19a5b87e0dd90fe40fb48ba38362f961be14e4f/wandb-0.8.31-py2.py3-none-any.whl (1.4MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \r\n",
      "\u001b[?25hCollecting watchdog>=0.8.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 7.8MB/s \r\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Collecting configparser>=3.8.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\r\n",
      "Collecting shortuuid>=0.5.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\r\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (3.0.5)\r\n",
      "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.352.0)\r\n",
      "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.6.5)\r\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.1.2)\r\n",
      "Collecting subprocess32>=3.5.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 6.6MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.22.0)\r\n",
      "Collecting sentry-sdk>=0.4.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\r\n",
      "\u001b[K     |████████████████████████████████| 112kB 19.2MB/s \r\n",
      "\u001b[?25hCollecting gql==0.2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.8.0)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (1.13.0)\r\n",
      "Requirement already satisfied, skipping upgrade: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.0)\r\n",
      "Collecting pathtools>=0.1.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (2.0.6)\r\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (2019.9.11)\r\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (2.8)\r\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (3.0.4)\r\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (1.24.2)\r\n",
      "Collecting graphql-core<2,>=0.5.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\r\n",
      "\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \r\n",
      "\u001b[?25hCollecting promise<3,>=2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\r\n",
      "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core, promise\r\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73604 sha256=9a648291fe5145088d5a347ba6bdc999ac17e6b37e9e2319dfdc4a80e0058cd2\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\r\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=6ba5fbe30b6bf35f9d40f920e41c6b703441bdab54259da200229d4021cf9e50\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\r\n",
      "  Building wheel for gql (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=f1b21a151f2dbcc2d83aa1e1b9dfcd063b92aa153d7b9c674853fc250ed12c29\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\r\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8786 sha256=64edc1e8d627222e3b022806c4c12c880ee157fabf504976b09220967e326ecf\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\r\n",
      "  Building wheel for graphql-core (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104651 sha256=d3b4371a6f0a6122aa233498fe32601c8c90a05f1b59fb5ffb27013909e5d535\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\r\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-cp36-none-any.whl size=21494 sha256=514521614ca0202e6b3288299dca45a1f6d8c87d5ae388efe10aaf3edc4e0cce\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\r\n",
      "Successfully built watchdog subprocess32 gql pathtools graphql-core promise\r\n",
      "Installing collected packages: pathtools, watchdog, docker-pycreds, configparser, shortuuid, subprocess32, sentry-sdk, promise, graphql-core, gql, wandb\r\n",
      "Successfully installed configparser-5.0.0 docker-pycreds-0.4.0 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 promise-2.3 sentry-sdk-0.14.3 shortuuid-1.0.1 subprocess32-3.5.4 wandb-0.8.31 watchdog-0.10.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login ee9416edde558c322450d0ec80266d2c0db81f45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/6d/47d8f50378b0b1edca8f98dc1628420870e076b685214409c6c88caced64/nlpaug-0.0.13.3-py3-none-any.whl (96kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 2.3MB/s \r\n",
      "\u001b[?25hInstalling collected packages: nlpaug\r\n",
      "Successfully installed nlpaug-0.0.13.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/fixed-embeddings\" target=\"_blank\">https://app.wandb.ai/arinaruck/fixed-embeddings</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/fixed-embeddings/runs/3110qqia\" target=\"_blank\">https://app.wandb.ai/arinaruck/fixed-embeddings/runs/3110qqia</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/paragram-300-sl999/paragram_300_sl999.txt\n",
      "/kaggle/input/augmented/augmented.csv\n",
      "/kaggle/input/augmented-fairseq/augmented (1).csv\n",
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import wandb\n",
    "#wandb.init(project=\"single model\", name='fixed embed synonym 0.25')\n",
    "wandb.init(project=\"fixed-embeddings\", name='synonym 0.25')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/quora-insincere-questions-classification/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "data_path = '/kaggle/input/quora-insincere-questions-classification/'\n",
    "seed = 2077\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/working/wandb/settings\n",
      "/kaggle/working/wandb/run-20200405_131503-3110qqia/wandb-history.jsonl\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_prob = 0.25\n",
    "# Preprocessing\n",
    "max_len = 50\n",
    "lower = True\n",
    "trunc = 'pre'\n",
    "max_features = 120000\n",
    "n_vocab = max_features\n",
    "clean_num = 0\n",
    "\n",
    "# Training\n",
    "aug_epochs = 4\n",
    "batch_size = 512\n",
    "drop_last = True\n",
    "dropout = 0.35\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "# Embedding\n",
    "fix_embedding = True\n",
    "unk_uni = True  # Initializer for unknown words делает вектор для незнакомого слова из N(emb.mean(), emb.std())\n",
    "n_embed = 2\n",
    "embed_dim = n_embed * 300 \n",
    "proj_dim = hidden_dim\n",
    "\n",
    "# GRU\n",
    "bidirectional = True\n",
    "n_layers = 1\n",
    "rnn_dim = hidden_dim\n",
    "\n",
    "# The second last Linear layer\n",
    "dense_dim = 2 * rnn_dim if bidirectional else rnn_dim\n",
    "\n",
    "# Test set\n",
    "test_batch_size = 8 * batch_size\n",
    "\n",
    "def seed_torch(seed=1):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_param_size(model, trainable=True):\n",
    "    if trainable:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n",
    "    else:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    return psize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z, _ = torch.max(inputs, 1)\n",
    "        return z\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim, dropout=0.2,\n",
    "                 padding_idx=0, pretrained_embedding=None, fix_embedding=True,\n",
    "                 n_out=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fix_embedding = fix_embedding\n",
    "        self.padding_idx = padding_idx\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(pretrained_embedding, freeze=fix_embedding)\n",
    "            self.embed.padding_idx = self.padding_idx\n",
    "        else:\n",
    "            self.embed = nn.Embedding(self.n_vocab, self.embed_dim, padding_idx=self.padding_idx)\n",
    "        self.embed_drop = nn.Dropout(dropout + 0.1)\n",
    "        self.linear_drop1 = nn.Dropout(dropout)\n",
    "        self.linear_drop2 = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_dim, rnn_dim, self.n_layers,\n",
    "                          batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.pooling = GlobalMaxPooling1D()\n",
    "        in_dim = 2 * rnn_dim if self.bidirectional else rnn_dim\n",
    "        self.dense = nn.Linear(in_dim, dense_dim)\n",
    "        self.dense_act = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(dense_dim)\n",
    "        self.out_linear = nn.Linear(dense_dim, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (bs, max_len)\n",
    "        x = self.embed(inputs)\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.linear_drop1(x)\n",
    "        x = self.dense_act(self.batchnorm(self.dense(x)))\n",
    "        x = self.linear_drop2(x)\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, dataloader, valloader, epochs, optimizer, scheduler, val_ans, patience=3, callbacks=None):\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "        for i in range(epochs):\n",
    "            print('epoch: ', i + 1)\n",
    "            run_epoch(model, dataloader, valloader, optimizer, scheduler, val_ans, early_stopping, callbacks=callbacks)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                X_batch = batch[0]\n",
    "                preds.append(self.forward(X_batch).data.cpu())\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def predict_proba(self, dataloader):\n",
    "        return torch.sigmoid(self.predict(dataloader)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    token_ids_s : List of token ids\n",
    "    labels   : Target labels\n",
    "    training: Sort token_ids_s by length if training is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_ids_s, labels=None, max_len=1000, training=True, sort=True):\n",
    "        self.training = training\n",
    "\n",
    "        if labels is None:\n",
    "            self.labels = torch.ones(len(token_ids_s), dtype=torch.long)  # dummy\n",
    "        else:\n",
    "            self.labels = torch.LongTensor(labels)\n",
    "\n",
    "        seq_lens = []\n",
    "        self.inputs = []\n",
    "        for e, token_ids in enumerate(token_ids_s):\n",
    "            seq_lens.append(len(token_ids))\n",
    "            input_ids = torch.LongTensor(token_ids[:max_len])\n",
    "            self.inputs.append(input_ids)\n",
    "\n",
    "        if self.training and sort:\n",
    "            self.indices = np.argsort(seq_lens)\n",
    "            self.inputs = [self.inputs[i] for i in self.indices]\n",
    "            self.labels = self.labels[self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Always drop last\n",
    "class BatchIterator(object):\n",
    "\n",
    "    def __init__(self, dataset, collate_fn, batch_size,\n",
    "                 shuffle=True, drop_last=True):\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(dataset)\n",
    "        self.shuffle = shuffle\n",
    "        if drop_last:\n",
    "            self.num_batches = self.size // batch_size\n",
    "        else:\n",
    "            self.num_batches = (self.size + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.random.choice(self.num_batches, self.num_batches, replace=False)\n",
    "        else:\n",
    "            indices = range(self.num_batches)\n",
    "        for idx in indices:\n",
    "            left = self.batch_size * idx\n",
    "            yield(self.collate_fn(self.dataset[left: left + self.batch_size]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xy_batch = [pad_sequence(batch[0], batch_first=True), batch[1]]\n",
    "    return xy_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, valloader, optimizer, scheduler, val_ans, early_stopping, callbacks=None,\n",
    "              criterion=nn.BCEWithLogitsLoss()):\n",
    "    t1 = time.time()\n",
    "    tr_loss = 0\n",
    "    tr_roc_auc = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs[:, 0], y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        tr_roc_auc += roc_auc_score(y_batch.cpu(), torch.sigmoid(outputs).data.cpu().numpy())\n",
    "        if callbacks is not None:\n",
    "            for func in callbacks:\n",
    "                func.on_batch_end(model)\n",
    "    tr_roc_auc /= step\n",
    "    tr_loss /= step\n",
    "    model.eval()\n",
    "    val_preds = model.predict(valloader)\n",
    "    val_ans = torch.tensor(val_ans.values, dtype=torch.float64)\n",
    "    val_loss = criterion(val_preds[:, 0], val_ans).item()\n",
    "    val_roc_auc = roc_auc_score(val_ans.int().cpu(), torch.sigmoid(val_preds).data.cpu().numpy())\n",
    "    print(f'val loss:{val_loss:.7f}')\n",
    "    print(f'train loss:{tr_loss:.7f}')\n",
    "    early_stopping(val_loss, model)    \n",
    "    if callbacks is not None:\n",
    "        for func in callbacks:\n",
    "            func.on_epoch_end(model)\n",
    "    wandb.log({\"loss/train\": tr_loss})\n",
    "    wandb.log({\"loss/val\": val_loss})\n",
    "    wandb.log({\"roc_auc/train\": tr_roc_auc})\n",
    "    wandb.log({\"roc_auc/val\": val_roc_auc})\n",
    "    scheduler.step(val_loss)\n",
    "    return tr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns glove', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_wiki(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/working/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE) if len(o) > 100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns wiki', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_parag(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/working/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' '))\n",
    "                            for o in open(EMBEDDING_FILE, encoding='utf8', errors='ignore')\n",
    "                            if len(o) > 100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns parag', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def load_ggle(word_index, max_features, unk_uni):\n",
    "    EMBEDDING_FILE = data_path + 'embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "    embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "    embed_size = embeddings_index.get_vector('known').size\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in embeddings_index:\n",
    "                embedding_matrix[i] = embeddings_index.get_vector(word_lower)\n",
    "            else:\n",
    "                unknown_words.append((word, i))\n",
    "\n",
    "    print('\\nTotal unknowns ggle', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = ',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║\\\n",
    "―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n",
    "\n",
    "\n",
    "def clean_text(x, puncts=puncts): #добавляет пробелы вокруг пунктуации\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(train_df, val_df, test_df, max_len, max_features, trunc='pre',\n",
    "                 lower=False, clean_num=2, unk_uni=True):\n",
    "    train_df = train_df.copy()\n",
    "    train_y = train_df['target'].values\n",
    "\n",
    "\n",
    "    # fill up the missing values\n",
    "    train_df['text'] = train_df['text'].fillna('_##_')\n",
    "    val_df['text'] = val_df['text'].fillna('_##_')\n",
    "    test_df['text'] = test_df['text'].fillna('_##_')\n",
    "\n",
    "    train_X = train_df['text'].values\n",
    "    val_X = val_df['text'].values\n",
    "    test_X = test_df['text'].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n",
    "    tokenizer.fit_on_texts(train_X)\n",
    "    print(tokenizer)\n",
    "    print('len(vocab)', len(tokenizer.word_index))\n",
    "\n",
    "    train_X_ids = tokenizer.texts_to_sequences(train_X)\n",
    "    train_dataset = TextClassificationDataset(train_X_ids, train_y, sort=False)\n",
    "    train_loader = BatchIterator(train_dataset, collate_fn, batch_size)\n",
    "    \n",
    "    val_X_ids = tokenizer.texts_to_sequences(val_X)\n",
    "    val_dataset = TextClassificationDataset(val_X_ids, training=False)\n",
    "    val_loader = BatchIterator(val_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "    test_X_ids = tokenizer.texts_to_sequences(test_X)\n",
    "    test_dataset = TextClassificationDataset(test_X_ids, training=False)\n",
    "    test_loader = BatchIterator(test_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "    return train_loader, val_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path  + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmented(p=0.35):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet', aug_p=p)\n",
    "    augmented_list = aug.augments(oneline['text'].values)\n",
    "    augmented = pd.DataFrame({'text' : augmented_list, \n",
    "                          'target' : list(oneline['target'].values)})\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns={'question_text': 'text'})\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(str.lower)\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "train_df['text'] = train_df['text'].apply(clean_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df.copy(), train_size=0.7, random_state=seed)\n",
    "oneline = train_df[~train_df['text'].str.contains('\\n')]\n",
    "multiline = train_df[train_df['text'].str.contains('\\n')]\n",
    "oneline_size = len(oneline)\n",
    "train_idx, valid_idx = train_test_split(np.arange(oneline_size), train_size=0.9, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = make_augmented(swap_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when volition men and boy find out the truth a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wherefore does not keralites  (  india  )  doe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will you stock still incur a cash advance fee ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what are the literal advantages of use quora  ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how much coffee is safe to toast  ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  when volition men and boy find out the truth a...       1\n",
       "1  wherefore does not keralites  (  india  )  doe...       0\n",
       "2  will you stock still incur a cash advance fee ...       0\n",
       "3   what are the literal advantages of use quora  ?        0\n",
       "4               how much coffee is safe to toast  ?        0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented['text'] = augmented['text'].apply(str.lower)\n",
    "augmented['text'] = augmented['text'].apply(clean_text)\n",
    "augmented['text'] = augmented['text'].apply(clean_numbers)\n",
    "augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>487207</th>\n",
       "      <td>5f6988e5cb1096691086</td>\n",
       "      <td>when will men and boys learn the truth and go ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113645</th>\n",
       "      <td>da36c0a619e1cf3fcdd4</td>\n",
       "      <td>why does not keralites  ( india )  does not li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558619</th>\n",
       "      <td>6d73fed2c64a4883e27b</td>\n",
       "      <td>will you still incur a cash advance fee if you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807057</th>\n",
       "      <td>9e22ba5c0a04d9a44236</td>\n",
       "      <td>what are the real advantages of using quora ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163578</th>\n",
       "      <td>e4006d45becca4c20ab7</td>\n",
       "      <td>how much coffee is safe to drink ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "487207   5f6988e5cb1096691086   \n",
       "1113645  da36c0a619e1cf3fcdd4   \n",
       "558619   6d73fed2c64a4883e27b   \n",
       "807057   9e22ba5c0a04d9a44236   \n",
       "1163578  e4006d45becca4c20ab7   \n",
       "\n",
       "                                                      text  target  \n",
       "487207   when will men and boys learn the truth and go ...       1  \n",
       "1113645  why does not keralites  ( india )  does not li...       0  \n",
       "558619   will you still incur a cash advance fee if you...       0  \n",
       "807057      what are the real advantages of using quora ?        0  \n",
       "1163578                how much coffee is safe to drink ?        0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = pd.DataFrame({\n",
    "    'text' : \n",
    "    list(oneline.iloc[train_idx]['text'].values) +\n",
    "    list(augmented.iloc[train_idx]['text'].values) +\n",
    "    list(multiline['text'].values),\n",
    "    'target' :\n",
    "    list(oneline.iloc[train_idx]['target'].values) +\n",
    "    list(augmented.iloc[train_idx]['target'].values) +\n",
    "    list(multiline['target'].values)\n",
    "})\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'text' : \n",
    "    list(oneline.iloc[train_idx]['text'].values) +\n",
    "    list(multiline['text'].values),\n",
    "    'target' :\n",
    "    list(oneline.iloc[train_idx]['target'].values) +\n",
    "    list(multiline['target'].values)\n",
    "})\n",
    "\n",
    "valid_df = oneline.iloc[valid_idx]\n",
    "\n",
    "train_aug.drop_duplicates(inplace=True)\n",
    "train_aug = train_aug.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822857 1645504\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.index),  len(train_aug.drop_duplicates().index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del augmented\n",
    "del train_df\n",
    "del oneline\n",
    "del multiline\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  (1645504, 2)\n",
      "Validition :  (91428, 3)\n",
      "Test:  (391837, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train : ', train_aug.shape)\n",
    "print('Validition : ', valid_df.shape)\n",
    "print('Test: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7fa025f0f9e8>\n",
      "len(vocab) 156983\n"
     ]
    }
   ],
   "source": [
    "data_path = '/kaggle/working/'\n",
    "obj = prepare_data(train_aug, valid_df, test_df, max_len, max_features,\n",
    "                   trunc=trunc, lower=lower, clean_num=clean_num, unk_uni=unk_uni)\n",
    "\n",
    "train_loader, val_loader, test_loader, tokenizer = obj\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unknowns glove 30331\n",
      "[('chenyu', 119981), ('《', 119982), ('》', 119983), ('canil', 119986), ('alturistic', 119987), ('tidepods', 119988), ('ilzamar', 119989), ('0bc', 119991), ('electeical', 119996), ('suvorov', 119998)]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_1, _ = load_glove(word_index, max_features, unk_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /kaggle/input/quora-insincere-questions-classification/embeddings.zip\r\n",
      "   creating: /kaggle/working/embeddings/wiki-news-300d-1M/\r\n",
      "  inflating: /kaggle/working/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec  \r\n",
      "\n",
      "Total unknowns wiki 46604\n",
      "[('tidepods', 119988), ('ilzamar', 119989), ('kubotan', 119990), ('0bc', 119991), ('plcc', 119992), ('unty', 119993), ('cameltoes', 119995), ('electeical', 119996), ('suvorov', 119998), ('ffer', 119999)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'wiki-news-300d-1M/*' -d /kaggle/working/embeddings/\n",
    "embedding_matrix_2, _ = load_wiki(word_index, max_features, unk_uni)\n",
    "!rm -r /kaggle/working/embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_matrix = np.concatenate((embedding_matrix_1,\n",
    "                                   embedding_matrix_2),\n",
    "                                   axis=1)\n",
    "del embedding_matrix_2,embedding_matrix_1\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.Tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 627201\n"
     ]
    }
   ],
   "source": [
    "val_ans, test_ans = valid_df['target'], test_df['target']\n",
    "\n",
    "model = GRUModel(n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim, pretrained_embedding=embedding_matrix,\n",
    "                 fix_embedding=fix_embedding, padding_idx=0)\n",
    "print(model)\n",
    "print('#Trainable params', get_param_size(model))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =  {\n",
    "            'model': 'bigru',\n",
    "            'optimizer': str(optimizer),\n",
    "            'lr' : 0.0005,\n",
    "            'scheduler': 'plateau',\n",
    "            'early_stop': 3,\n",
    "            'vocab_size': n_vocab,\n",
    "             'h_size': hidden_dim,\n",
    "             'n_layers': n_layers,\n",
    "             'dropout': dropout,\n",
    "             'batch_size': batch_size,\n",
    "             'embed_dim': embed_dim,\n",
    "             'max_len': max_len,\n",
    "             'batch_norm' : 'true'\n",
    "        }\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "val loss:0.1052508\n",
      "train loss:0.1227207\n",
      "Validation loss decreased (inf --> 0.105251).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1010862\n",
      "train loss:0.1016473\n",
      "Validation loss decreased (0.105251 --> 0.101086).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.0980129\n",
      "train loss:0.0948945\n",
      "Validation loss decreased (0.101086 --> 0.098013).  Saving model ...\n",
      "epoch:  4\n",
      "val loss:0.0976202\n",
      "train loss:0.0889930\n",
      "Validation loss decreased (0.098013 --> 0.097620).  Saving model ...\n",
      "n_model:1 87.5s/epoch\n",
      "n_model:1 0.9681882394458354\n",
      "10.6s\n"
     ]
    }
   ],
   "source": [
    "val_probas = np.zeros((len(valid_df), 1))\n",
    "test_probas = np.zeros((len(test_df), 1))\n",
    "\n",
    "t2 = time.time()\n",
    "wandb.watch(model)\n",
    "model.fit(train_loader, val_loader, aug_epochs, optimizer, scheduler, val_ans)\n",
    "print(f'n_model:{1} {(time.time() - t2) / (aug_epochs):.1f}s/epoch')\n",
    "t3 = time.time()\n",
    "val_probas =  model.predict_proba(val_loader)\n",
    "test_probas = model.predict_proba(test_loader)\n",
    "print(f'n_model:{1} {roc_auc_score(val_ans, val_probas)}')\n",
    "print(f'{time.time() - t3:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4093749999999999\n"
     ]
    }
   ],
   "source": [
    "import scipy \n",
    "\n",
    "res = scipy.optimize.minimize(\n",
    "    lambda t: -f1_score(val_ans, (val_probas >= t).astype(np.int)),\n",
    "    x0=0.5,\n",
    "    method='Nelder-Mead',\n",
    "    tol=1e-3,\n",
    ")\n",
    "best_threshold = res.x[0]\n",
    "print(best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1-score: 0.6853124627547574\n",
      "test roc-auc score: 0.9677078487684749\n"
     ]
    }
   ],
   "source": [
    "f1_val = f1_score(val_ans, (val_probas >= best_threshold))\n",
    "f1_test = f1_score(test_ans, test_probas >= best_threshold)\n",
    "wandb.log({'f1/val' : f1_val})\n",
    "wandb.log({'f1/test' : f1_test})\n",
    "roc_auc = roc_auc_score(test_ans, test_probas)\n",
    "print('test f1-score: {}'.format(f1_test))\n",
    "print('test roc-auc score: {}'.format(roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
