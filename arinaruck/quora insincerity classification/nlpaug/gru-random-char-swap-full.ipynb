{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 4.4MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.22.0)\r\n",
      "Collecting watchdog>=0.8.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 8.9MB/s \r\n",
      "\u001b[?25hCollecting sentry-sdk>=0.4.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\r\n",
      "\u001b[K     |████████████████████████████████| 112kB 23.0MB/s \r\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\r\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (3.0.5)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (1.13.0)\r\n",
      "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.6.5)\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.8.0)\r\n",
      "Collecting subprocess32>=3.5.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 8.2MB/s \r\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Collecting gql==0.2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\r\n",
      "Collecting configparser>=3.8.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.1.2)\r\n",
      "Requirement already satisfied, skipping upgrade: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.0)\r\n",
      "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.352.0)\r\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (1.24.2)\r\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (2019.9.11)\r\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (3.0.4)\r\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (2.8)\r\n",
      "Collecting pathtools>=0.1.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (2.0.6)\r\n",
      "Collecting graphql-core<2,>=0.5.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\r\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \r\n",
      "\u001b[?25hCollecting promise<3,>=2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\r\n",
      "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core, promise\r\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73604 sha256=7dd2bed048b80fefd719f03ecf7eb170ebdf1d4768dfe4d450f290a288ae33b6\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\r\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=34850726f8c19945a1c2dea20ffd9d763fdff70cdbe5fbd455b7fbc46f9f1300\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\r\n",
      "  Building wheel for gql (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=677b5e4bf9d5655b69b125aaafde45b82f47c4642e041cd128bb24657e31bb9f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\r\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8786 sha256=72f7b595318be634ab8f597d239627744d4da05862e59be92a295c6733a0b43e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\r\n",
      "  Building wheel for graphql-core (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104651 sha256=7f6ad23e83b990990200ec58077215070d9bdc1e74cf273a54bce1763ebfa4d7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\r\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-cp36-none-any.whl size=21494 sha256=17cdce5b5b238c219d8b0fdd0935aa74b1120030edfee316c94fcd642c9684e0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\r\n",
      "Successfully built watchdog subprocess32 gql pathtools graphql-core promise\r\n",
      "Installing collected packages: pathtools, watchdog, sentry-sdk, shortuuid, subprocess32, docker-pycreds, promise, graphql-core, gql, configparser, wandb\r\n",
      "Successfully installed configparser-5.0.0 docker-pycreds-0.4.0 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 promise-2.3 sentry-sdk-0.14.3 shortuuid-1.0.1 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login ee9416edde558c322450d0ec80266d2c0db81f45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/6c/ca85b6bd29926561229e8c9f677c36c65db9ef1947bfc175e6641bc82ace/nlpaug-0.0.14-py3-none-any.whl (101kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 3.5MB/s \r\n",
      "\u001b[?25hInstalling collected packages: nlpaug\r\n",
      "Successfully installed nlpaug-0.0.14\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/paragram-300-sl999/paragram_300_sl999.txt\n",
      "/kaggle/input/augmented/augmented.csv\n",
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/augmented-fairseq/augmented (1).csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/quora-insincere-questions-classification/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "data_path = '/kaggle/input/quora-insincere-questions-classification/'\n",
    "seed = 2077\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/working/wandb/settings\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_prob = 0.5\n",
    "# Preprocessing\n",
    "max_len = 50\n",
    "lower = True\n",
    "trunc = 'pre'\n",
    "max_features = 120000\n",
    "n_vocab = max_features\n",
    "clean_num = 0\n",
    "\n",
    "# Training\n",
    "aug_epochs = 5\n",
    "batch_size = 512\n",
    "drop_last = True\n",
    "dropout = 0.35\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "# Embedding\n",
    "fix_embedding = True\n",
    "unk_uni = True  # Initializer for unknown words делает вектор для незнакомого слова из N(emb.mean(), emb.std())\n",
    "n_embed = 2\n",
    "embed_dim = n_embed * 300 \n",
    "proj_dim = hidden_dim\n",
    "\n",
    "# GRU\n",
    "bidirectional = True\n",
    "n_layers = 1\n",
    "rnn_dim = hidden_dim\n",
    "\n",
    "# The second last Linear layer\n",
    "dense_dim = 2 * rnn_dim if bidirectional else rnn_dim\n",
    "\n",
    "# Test set\n",
    "test_batch_size = 8 * batch_size\n",
    "\n",
    "def seed_torch(seed=1):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_param_size(model, trainable=True):\n",
    "    if trainable:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n",
    "    else:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    return psize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z, _ = torch.max(inputs, 1)\n",
    "        return z\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim, dropout=0.2,\n",
    "                 padding_idx=0, pretrained_embedding=None, fix_embedding=True,\n",
    "                 n_out=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fix_embedding = fix_embedding\n",
    "        self.padding_idx = padding_idx\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(pretrained_embedding, freeze=fix_embedding)\n",
    "            self.embed.padding_idx = self.padding_idx\n",
    "        else:\n",
    "            self.embed = nn.Embedding(self.n_vocab, self.embed_dim, padding_idx=self.padding_idx)\n",
    "        self.embed_drop = nn.Dropout(dropout + 0.1)\n",
    "        self.linear_drop1 = nn.Dropout(dropout)\n",
    "        self.linear_drop2 = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_dim, rnn_dim, self.n_layers,\n",
    "                          batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.pooling = GlobalMaxPooling1D()\n",
    "        in_dim = 2 * rnn_dim if self.bidirectional else rnn_dim\n",
    "        self.dense = nn.Linear(in_dim, dense_dim)\n",
    "        self.dense_act = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(dense_dim)\n",
    "        self.out_linear = nn.Linear(dense_dim, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (bs, max_len)\n",
    "        x = self.embed_drop(self.embed(inputs))\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.linear_drop1(x)\n",
    "        x = self.dense_act(self.batchnorm(self.dense(x)))\n",
    "        x = self.linear_drop2(x)\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, dataloader, valloader, epochs, optimizer, scheduler, val_ans, patience=3, callbacks=None):\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "        for i in range(epochs):\n",
    "            print('epoch: ', i + 1)\n",
    "            self.run_epoch(dataloader, valloader, optimizer, scheduler, val_ans, early_stopping, callbacks=callbacks)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                X_batch = batch[0]\n",
    "                preds.append(self.forward(X_batch).data.cpu())\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def predict_proba(self, dataloader):\n",
    "        return torch.sigmoid(self.predict(dataloader)).data.numpy()\n",
    "    \n",
    "    def run_epoch(self, dataloader, valloader, optimizer, scheduler, val_ans, early_stopping, callbacks=None,\n",
    "              criterion=nn.BCEWithLogitsLoss()):\n",
    "        t1 = time.time()\n",
    "        tr_loss = 0\n",
    "        tr_roc_auc = 0\n",
    "        self.train()\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            x_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.forward(x_batch)\n",
    "            loss = criterion(outputs[:, 0], y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()\n",
    "            tr_roc_auc += roc_auc_score(y_batch.cpu(), torch.sigmoid(outputs).data.cpu().numpy())\n",
    "            if callbacks is not None:\n",
    "                for func in callbacks:\n",
    "                    func.on_batch_end(self)\n",
    "        tr_roc_auc /= step\n",
    "        tr_loss /= step\n",
    "        self.eval()\n",
    "        val_preds = self.predict(valloader)\n",
    "        val_ans = torch.tensor(val_ans.values, dtype=torch.float64)\n",
    "        val_loss = criterion(val_preds[:, 0], val_ans).item()\n",
    "        val_roc_auc = roc_auc_score(val_ans.int().cpu(), torch.sigmoid(val_preds).data.cpu().numpy())\n",
    "        print(f'val loss:{val_loss:.7f}')\n",
    "        print(f'train loss:{tr_loss:.7f}')\n",
    "        early_stopping(val_loss, self)    \n",
    "        if callbacks is not None:\n",
    "            for func in callbacks:\n",
    "                func.on_epoch_end(self)\n",
    "        wandb.log({\"loss/train\": tr_loss})\n",
    "        wandb.log({\"loss/val\": val_loss})\n",
    "        wandb.log({\"roc_auc/train\": tr_roc_auc})\n",
    "        wandb.log({\"roc_auc/val\": val_roc_auc})\n",
    "        scheduler.step(val_loss)\n",
    "        return tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    token_ids_s : List of token ids\n",
    "    labels   : Target labels\n",
    "    training: Sort token_ids_s by length if training is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_ids_s, labels=None, max_len=1000, training=True, sort=True):\n",
    "        self.training = training\n",
    "\n",
    "        if labels is None:\n",
    "            self.labels = torch.ones(len(token_ids_s), dtype=torch.long)  # dummy\n",
    "        else:\n",
    "            self.labels = torch.LongTensor(labels)\n",
    "\n",
    "        seq_lens = []\n",
    "        self.inputs = []\n",
    "        for e, token_ids in enumerate(token_ids_s):\n",
    "            seq_lens.append(len(token_ids))\n",
    "            input_ids = torch.LongTensor(token_ids[:max_len])\n",
    "            self.inputs.append(input_ids)\n",
    "\n",
    "        if self.training and sort:\n",
    "            self.indices = np.argsort(seq_lens)\n",
    "            self.inputs = [self.inputs[i] for i in self.indices]\n",
    "            self.labels = self.labels[self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Always drop last\n",
    "class BatchIterator(object):\n",
    "\n",
    "    def __init__(self, dataset, collate_fn, batch_size,\n",
    "                 shuffle=True, drop_last=True):\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(dataset)\n",
    "        self.shuffle = shuffle\n",
    "        if drop_last:\n",
    "            self.num_batches = self.size // batch_size\n",
    "        else:\n",
    "            self.num_batches = (self.size + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.random.choice(self.num_batches, self.num_batches, replace=False)\n",
    "        else:\n",
    "            indices = range(self.num_batches)\n",
    "        for idx in indices:\n",
    "            left = self.batch_size * idx\n",
    "            yield(self.collate_fn(self.dataset[left: left + self.batch_size]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xy_batch = [pad_sequence(batch[0], batch_first=True), batch[1]]\n",
    "    return xy_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = ',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║\\\n",
    "―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n",
    "\n",
    "\n",
    "def clean_text(x, puncts=puncts): #добавляет пробелы вокруг пунктуации\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(train_df, val_df, test_df, max_len, max_features, trunc='pre',\n",
    "                 lower=False, clean_num=2, unk_uni=True):\n",
    "    train_df = train_df.copy()\n",
    "    train_y = train_df['target'].values\n",
    "\n",
    "\n",
    "    # fill up the missing values\n",
    "    train_df['text'] = train_df['text'].fillna('_##_')\n",
    "    val_df['text'] = val_df['text'].fillna('_##_')\n",
    "    test_df['text'] = test_df['text'].fillna('_##_')\n",
    "\n",
    "    train_X = train_df['text'].values\n",
    "    val_X = val_df['text'].values\n",
    "    test_X = test_df['text'].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n",
    "    tokenizer.fit_on_texts(train_X)\n",
    "    print(tokenizer)\n",
    "    print('len(vocab)', len(tokenizer.word_index))\n",
    "\n",
    "    train_X_ids = tokenizer.texts_to_sequences(train_X)\n",
    "    train_dataset = TextClassificationDataset(train_X_ids, train_y, sort=False)\n",
    "    train_loader = BatchIterator(train_dataset, collate_fn, batch_size)\n",
    "    \n",
    "    val_X_ids = tokenizer.texts_to_sequences(val_X)\n",
    "    val_dataset = TextClassificationDataset(val_X_ids, training=False)\n",
    "    val_loader = BatchIterator(val_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "    test_X_ids = tokenizer.texts_to_sequences(test_X)\n",
    "    test_dataset = TextClassificationDataset(test_X_ids, training=False)\n",
    "    test_loader = BatchIterator(test_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "    return train_loader, val_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path  + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmented(p=0.35):\n",
    "    aug = nac.RandomCharAug(action='swap', aug_word_p=p)\n",
    "    augmented_list = aug.augments(oneline['text'].values)\n",
    "    augmented = pd.DataFrame({'text' : augmented_list, \n",
    "                          'target' : list(oneline['target'].values)})\n",
    "    return augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns={'question_text': 'text'})\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(str.lower)\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "train_df['text'] = train_df['text'].apply(clean_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df.copy(), train_size=0.7, random_state=seed)\n",
    "oneline = train_df[~train_df['text'].str.contains('\\n')]\n",
    "multiline = train_df[train_df['text'].str.contains('\\n')]\n",
    "oneline_size = len(oneline)\n",
    "train_idx, valid_idx = train_test_split(np.arange(oneline_size), train_size=0.9, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(aug_prob, epochs):\n",
    "    wandb.init(project=\"trained embeddings\", name=f'char swap {aug_prob}',reinit=True)\n",
    "    #wandb.init(project=\"single model\", name=f'trained char swap {aug_prob}',reinit=True)\n",
    "    \n",
    "    augmented = make_augmented(aug_prob)\n",
    "    \n",
    "    augmented['text'] = augmented['text'].apply(str.lower)\n",
    "    augmented['text'] = augmented['text'].apply(clean_text)\n",
    "    augmented['text'] = augmented['text'].apply(clean_numbers)\n",
    "    \n",
    "    \n",
    "    train_aug = pd.DataFrame({\n",
    "        'text' : \n",
    "        list(oneline.iloc[train_idx]['text'].values) +\n",
    "        list(augmented.iloc[train_idx]['text'].values) +\n",
    "        list(multiline['text'].values),\n",
    "        'target' :\n",
    "        list(oneline.iloc[train_idx]['target'].values) +\n",
    "        list(augmented.iloc[train_idx]['target'].values) +\n",
    "        list(multiline['target'].values)\n",
    "    })\n",
    "\n",
    "\n",
    "    valid_df = oneline.iloc[valid_idx]\n",
    "\n",
    "    train_aug.drop_duplicates(inplace=True)\n",
    "    train_aug = train_aug.sample(frac=1)\n",
    "    \n",
    "    #del augmented\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    data_path = '/kaggle/working/'\n",
    "    obj = prepare_data(train_aug, valid_df, test_df, max_len, max_features,\n",
    "                   trunc=trunc, lower=lower, clean_num=clean_num, unk_uni=unk_uni)\n",
    "\n",
    "    train_loader, val_loader, test_loader, tokenizer = obj\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    val_ans, test_ans = valid_df['target'], test_df['target']\n",
    "\n",
    "    model = GRUModel(n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim, padding_idx=0)\n",
    "    print(model)\n",
    "    print('#Trainable params', get_param_size(model))\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.0005)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=2, factor=0.5)\n",
    "    \n",
    "    config =  {\n",
    "            'model': 'bigru',\n",
    "            'optimizer': str(optimizer),\n",
    "            'lr' : 0.0005,\n",
    "            'scheduler': 'plateau',\n",
    "            'early_stop': 3,\n",
    "            'vocab_size': n_vocab,\n",
    "             'h_size': hidden_dim,\n",
    "             'n_layers': n_layers,\n",
    "             'dropout': dropout,\n",
    "             'batch_size': batch_size,\n",
    "             'embed_dim': embed_dim,\n",
    "             'max_len': max_len,\n",
    "             'batch_norm' : 'true'\n",
    "        }\n",
    "    wandb.config.update(config)\n",
    "    \n",
    "    val_probas = np.zeros((len(valid_df), 1))\n",
    "    test_probas = np.zeros((len(test_df), 1))\n",
    "\n",
    "    t2 = time.time()\n",
    "    wandb.watch(model)\n",
    "    model.fit(train_loader, val_loader, epochs, optimizer, scheduler, val_ans)\n",
    "    print(f'n_model:{1} {(time.time() - t2) / (aug_epochs):.1f}s/epoch')\n",
    "    t3 = time.time()\n",
    "    val_probas =  model.predict_proba(val_loader)\n",
    "    test_probas = model.predict_proba(test_loader)\n",
    "    print(f'n_model:{1} {roc_auc_score(val_ans, val_probas)}')\n",
    "    print(f'{time.time() - t3:.1f}s')\n",
    "\n",
    "    res = scipy.optimize.minimize(\n",
    "        lambda t: -f1_score(val_ans, (val_probas >= t).astype(np.int)),\n",
    "        x0=0.5,\n",
    "        method='Nelder-Mead',\n",
    "        tol=1e-3,\n",
    "    )\n",
    "    best_threshold = res.x[0]\n",
    "    print(best_threshold)\n",
    "    \n",
    "    f1_val = f1_score(val_ans, (val_probas >= best_threshold))\n",
    "    f1_test = f1_score(test_ans, test_probas >= best_threshold)\n",
    "    wandb.log({'f1/val' : f1_val})\n",
    "    wandb.log({'f1/test' : f1_test})\n",
    "    roc_auc = roc_auc_score(test_ans, test_probas)\n",
    "    print('test f1-score: {}'.format(f1_test))\n",
    "    print('test roc-auc score: {}'.format(roc_auc))\n",
    "    \n",
    "    wandb.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings/runs/zg2zwnkv\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings/runs/zg2zwnkv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7faa5f6e6908>\n",
      "len(vocab) 271637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1151465\n",
      "train loss:0.1381323\n",
      "Validation loss decreased (inf --> 0.115147).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1098086\n",
      "train loss:0.1109639\n",
      "Validation loss decreased (0.115147 --> 0.109809).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1086720\n",
      "train loss:0.1020999\n",
      "Validation loss decreased (0.109809 --> 0.108672).  Saving model ...\n",
      "epoch:  4\n",
      "val loss:0.1084213\n",
      "train loss:0.0955163\n",
      "Validation loss decreased (0.108672 --> 0.108421).  Saving model ...\n",
      "n_model:1 118.6s/epoch\n",
      "n_model:1 0.9583174143579763\n",
      "10.6s\n",
      "0.33828124999999987\n",
      "test f1-score: 0.6487837000309886\n",
      "test roc-auc score: 0.9574287628233136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings/runs/11c6naxc\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings/runs/11c6naxc</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7faa7a3c7ef0>\n",
      "len(vocab) 338666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1153652\n",
      "train loss:0.1442398\n",
      "Validation loss decreased (inf --> 0.115365).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1101311\n",
      "train loss:0.1138457\n",
      "Validation loss decreased (0.115365 --> 0.110131).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1097215\n",
      "train loss:0.1039369\n",
      "Validation loss decreased (0.110131 --> 0.109722).  Saving model ...\n",
      "n_model:1 89.0s/epoch\n",
      "n_model:1 0.9580135328217595\n",
      "11.4s\n",
      "0.3507812499999999\n",
      "test f1-score: 0.64522362223743\n",
      "test roc-auc score: 0.957793954077116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings/runs/1klbxl73\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings/runs/1klbxl73</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7faa4ad36e10>\n",
      "len(vocab) 401680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1159233\n",
      "train loss:0.1446700\n",
      "Validation loss decreased (inf --> 0.115923).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1092688\n",
      "train loss:0.1150827\n",
      "Validation loss decreased (0.115923 --> 0.109269).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1092544\n",
      "train loss:0.1050936\n",
      "Validation loss decreased (0.109269 --> 0.109254).  Saving model ...\n",
      "epoch:  4\n",
      "val loss:0.1092640\n",
      "train loss:0.0977828\n",
      "EarlyStopping counter: 1 out of 3\n",
      "n_model:1 118.9s/epoch\n",
      "n_model:1 0.9580138140523838\n",
      "11.3s\n",
      "0.2999999999999998\n",
      "test f1-score: 0.6467592243265676\n",
      "test roc-auc score: 0.957227581192255\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings/runs/2wuwhteh\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings/runs/2wuwhteh</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7faa4c7f8358>\n",
      "len(vocab) 455082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1183910\n",
      "train loss:0.1508451\n",
      "Validation loss decreased (inf --> 0.118391).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1110241\n",
      "train loss:0.1177334\n",
      "Validation loss decreased (0.118391 --> 0.111024).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1086745\n",
      "train loss:0.1069578\n",
      "Validation loss decreased (0.111024 --> 0.108675).  Saving model ...\n",
      "n_model:1 88.8s/epoch\n",
      "n_model:1 0.9590177064263842\n",
      "10.8s\n",
      "0.3156249999999998\n",
      "test f1-score: 0.6473771777170066\n",
      "test roc-auc score: 0.9583147945790276\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings/runs/5sxxpy2u\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings/runs/5sxxpy2u</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7faa718842e8>\n",
      "len(vocab) 501456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1197253\n",
      "train loss:0.1479896\n",
      "Validation loss decreased (inf --> 0.119725).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1100247\n",
      "train loss:0.1174036\n",
      "Validation loss decreased (0.119725 --> 0.110025).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1095976\n",
      "train loss:0.1072418\n",
      "Validation loss decreased (0.110025 --> 0.109598).  Saving model ...\n",
      "n_model:1 89.3s/epoch\n",
      "n_model:1 0.9576568829430641\n",
      "11.1s\n",
      "0.2984374999999998\n",
      "test f1-score: 0.6453984953900108\n",
      "test roc-auc score: 0.9574831569652934\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings/runs/10i4duhi\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings/runs/10i4duhi</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7faa17ffbda0>\n",
      "len(vocab) 528774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1171138\n",
      "train loss:0.1474073\n",
      "Validation loss decreased (inf --> 0.117114).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1098082\n",
      "train loss:0.1185614\n",
      "Validation loss decreased (0.117114 --> 0.109808).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1096317\n",
      "train loss:0.1087035\n",
      "Validation loss decreased (0.109808 --> 0.109632).  Saving model ...\n",
      "n_model:1 88.0s/epoch\n",
      "n_model:1 0.95726531823086\n",
      "10.8s\n",
      "0.2820312499999998\n",
      "test f1-score: 0.6456298805155707\n",
      "test roc-auc score: 0.9570873317286099\n"
     ]
    }
   ],
   "source": [
    "augs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "epochs_num = [4, 3, 4, 3, 3, 3]\n",
    "\n",
    "for aug_p, epochs in zip(augs, epochs_num):\n",
    "    train(aug_p, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
