{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting comet_ml\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/5c/80369905a5ff6f356be389b1f2b4f092ded3717f753d31f7c5a322c6a3f7/comet_ml-3.1.2-py2.py3-none-any.whl (183kB)\r\n",
      "\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (7.352.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from comet_ml) (1.13.0)\r\n",
      "Collecting netifaces>=0.10.7\r\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\r\n",
      "Collecting comet-git-pure>=0.19.11\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/2b/c1ca11a237e3fcbe0c1ea53134901f30f3aa657e5ea141dccaae0f46df0e/comet_git_pure-0.19.15-py3-none-any.whl (401kB)\r\n",
      "\u001b[K     |████████████████████████████████| 409kB 8.1MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (2.22.0)\r\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (2.0.0)\r\n",
      "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\r\n",
      "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (0.56.0)\r\n",
      "Collecting jsonschema<3.1.0,>=2.6.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/48/f5f11003ceddcd4ad292d4d9b5677588e9169eef41f88e38b2888e7ec6c4/jsonschema-3.0.2-py2.py3-none-any.whl (54kB)\r\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.24.1 in /opt/conda/lib/python3.6/site-packages (from comet-git-pure>=0.19.11->comet_ml) (1.24.2)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.6/site-packages (from comet-git-pure>=0.19.11->comet_ml) (2019.9.11)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (2.8)\r\n",
      "Collecting configobj; extra == \"ini\"\r\n",
      "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->comet_ml) (42.0.1.post20191125)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->comet_ml) (19.3.0)\r\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->comet_ml) (0.15.6)\r\n",
      "Building wheels for collected packages: configobj\r\n",
      "  Building wheel for configobj (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34546 sha256=85cf40a2c1980ae43ccf57c0fb75d69c699cb4b6a6044a484ce78f7d483d0aaa\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\r\n",
      "Successfully built configobj\r\n",
      "Installing collected packages: netifaces, comet-git-pure, configobj, everett, jsonschema, comet-ml\r\n",
      "  Found existing installation: jsonschema 3.2.0\r\n",
      "    Uninstalling jsonschema-3.2.0:\r\n",
      "      Successfully uninstalled jsonschema-3.2.0\r\n",
      "Successfully installed comet-git-pure-0.19.15 comet-ml-3.1.2 configobj-5.0.6 everett-1.0.2 jsonschema-3.0.2 netifaces-0.10.9\r\n",
      "Collecting wandb\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/6f/f8485396e1e17c6b4257eecbcae1869d4d27ec960eb2bc06ea3fb829bc9a/wandb-0.8.30-py2.py3-none-any.whl (1.4MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \r\n",
      "\u001b[?25hCollecting configparser>=3.8.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.352.0)\r\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.1.2)\r\n",
      "Collecting subprocess32>=3.5.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.6.5)\r\n",
      "Collecting gql==0.2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.8.0)\r\n",
      "Collecting sentry-sdk>=0.4.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\r\n",
      "\u001b[K     |████████████████████████████████| 112kB 18.8MB/s \r\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Collecting shortuuid>=0.5.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\r\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (3.0.5)\r\n",
      "Requirement already satisfied, skipping upgrade: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.0)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (1.13.0)\r\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.22.0)\r\n",
      "Collecting watchdog>=0.8.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 8.0MB/s \r\n",
      "\u001b[?25hCollecting graphql-core<2,>=0.5.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\r\n",
      "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \r\n",
      "\u001b[?25hCollecting promise<3,>=2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from sentry-sdk>=0.4.0->wandb) (1.24.2)\r\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.6/site-packages (from sentry-sdk>=0.4.0->wandb) (2019.9.11)\r\n",
      "Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (2.0.6)\r\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (3.0.4)\r\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (2.8)\r\n",
      "Collecting pathtools>=0.1.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\r\n",
      "Building wheels for collected packages: subprocess32, gql, watchdog, graphql-core, promise, pathtools\r\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=162f52d1de187e290a83703032f44dcd95111d20c15eac5bc50dbf6128a89bd4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\r\n",
      "  Building wheel for gql (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=04dc26ebdad416fd0bcdc5ef0879f63f33ff0dce4e20753bb7dfd27f86eff5c6\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\r\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73604 sha256=5db5216c9240906923109637b814a6b298bd8877b9029bd55c042321a2422bf8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\r\n",
      "  Building wheel for graphql-core (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104651 sha256=4d90d3fbb2181cf1148a50964e3288e30963902c52ee0a0ce37da4648cbad434\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\r\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-cp36-none-any.whl size=21494 sha256=3477f0084408c5141c204afcd22be2bdcb27c303471cfbcd7881776f0e47f5ad\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\r\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8786 sha256=1aa7d7008e3274d8e9748c092a550e08ff42687c88a2ffaf8dfe0685d2728d8b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\r\n",
      "Successfully built subprocess32 gql watchdog graphql-core promise pathtools\r\n",
      "Installing collected packages: configparser, subprocess32, promise, graphql-core, gql, sentry-sdk, docker-pycreds, shortuuid, pathtools, watchdog, wandb\r\n",
      "Successfully installed configparser-4.0.2 docker-pycreds-0.4.0 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 promise-2.3 sentry-sdk-0.14.3 shortuuid-1.0.1 subprocess32-3.5.4 wandb-0.8.30 watchdog-0.10.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install comet_ml\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login ee9416edde558c322450d0ec80266d2c0db81f45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/single%20model%20600dim\" target=\"_blank\">https://app.wandb.ai/arinaruck/single%20model%20600dim</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/single%20model%20600dim/runs/2ce5lovg\" target=\"_blank\">https://app.wandb.ai/arinaruck/single%20model%20600dim/runs/2ce5lovg</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/paragram-300-sl999/paragram_300_sl999.txt\n",
      "/kaggle/input/augmented-fairseq/augmented (1).csv\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/augmented/augmented.csv\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"single model 600dim\", name='embeddings 1,2 no aug')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/quora-insincere-questions-classification/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "data_path = '/kaggle/input/quora-insincere-questions-classification/'\n",
    "seed = 2077\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/working/wandb/settings\n",
      "/kaggle/working/wandb/run-20200322_111838-2ce5lovg/wandb-history.jsonl\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/comet-ml-testing/gru-classificaton/4587e5179cea4709abd5d910d887da31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api = 'GGF21Vtrnid3Cgat9n1nL9Vcc'\n",
    "experiment = Experiment(api_key=api, project_name=\"GRU classificaton\", workspace=\"comet-ml testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "max_len = 50\n",
    "lower = True\n",
    "trunc = 'pre'\n",
    "max_features = 120000\n",
    "n_vocab = max_features\n",
    "clean_num = 0\n",
    "\n",
    "# Training\n",
    "aug_epochs = 6\n",
    "batch_size = 512\n",
    "drop_last = True\n",
    "dropout = 0.35\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "# Embedding\n",
    "fix_embedding = True\n",
    "unk_uni = True  # Initializer for unknown words делает вектор для незнакомого слова из N(emb.mean(), emb.std())\n",
    "n_embed = 2\n",
    "embed_dim = n_embed * 300 \n",
    "proj_dim = hidden_dim\n",
    "\n",
    "# GRU\n",
    "bidirectional = True\n",
    "n_layers = 1\n",
    "rnn_dim = hidden_dim\n",
    "\n",
    "# The second last Linear layer\n",
    "dense_dim = 2 * rnn_dim if bidirectional else rnn_dim\n",
    "\n",
    "# Test set\n",
    "threshold = 0.353\n",
    "test_batch_size = 8 * batch_size\n",
    "\n",
    "def seed_torch(seed=1):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_param_size(model, trainable=True):\n",
    "    if trainable:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n",
    "    else:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    return psize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z, _ = torch.max(inputs, 1)\n",
    "        return z\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim, dropout=0.2,\n",
    "                 padding_idx=0, pretrained_embedding=None, fix_embedding=True,\n",
    "                 n_out=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fix_embedding = fix_embedding\n",
    "        self.padding_idx = padding_idx\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(pretrained_embedding, freeze=fix_embedding)\n",
    "            self.embed.padding_idx = self.padding_idx\n",
    "        else:\n",
    "            self.embed = nn.Embedding(self.n_vocab, self.embed_dim, padding_idx=self.padding_idx)\n",
    "        self.embed_drop = nn.Dropout(dropout)\n",
    "        self.linear_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, proj_dim)\n",
    "        self.proj_act = nn.ReLU()\n",
    "        self.gru = nn.GRU(proj_dim, rnn_dim, self.n_layers,\n",
    "                          batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.pooling = GlobalMaxPooling1D()\n",
    "        in_dim = 2 * rnn_dim if self.bidirectional else rnn_dim\n",
    "        self.dense = nn.Linear(in_dim, dense_dim)\n",
    "        self.dense_act = nn.ReLU()\n",
    "        self.out_linear = nn.Linear(dense_dim, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (bs, max_len)\n",
    "        x = self.embed(inputs)\n",
    "        x = self.proj_act(self.proj(x))\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.linear_drop(x)\n",
    "        x = self.dense_act(self.dense(x))\n",
    "        x = self.linear_drop(x)\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, dataloader, valloader, epochs, optimizer, scheduler, val_ans, callbacks=None):\n",
    "        for i in range(epochs):\n",
    "            run_epoch(model, dataloader, valloader, optimizer, scheduler, val_ans, callbacks=callbacks)\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                X_batch = batch[0]\n",
    "                preds.append(self.forward(X_batch).data.cpu())\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def predict_proba(self, dataloader):\n",
    "        return torch.sigmoid(self.predict(dataloader)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    token_ids_s : List of token ids\n",
    "    labels   : Target labels\n",
    "    training: Sort token_ids_s by length if training is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_ids_s, labels=None, max_len=1000, training=True, sort=True):\n",
    "        self.training = training\n",
    "\n",
    "        if labels is None:\n",
    "            self.labels = torch.ones(len(token_ids_s), dtype=torch.long)  # dummy\n",
    "        else:\n",
    "            self.labels = torch.LongTensor(labels)\n",
    "\n",
    "        seq_lens = []\n",
    "        self.inputs = []\n",
    "        for e, token_ids in enumerate(token_ids_s):\n",
    "            seq_lens.append(len(token_ids))\n",
    "            input_ids = torch.LongTensor(token_ids[:max_len])\n",
    "            self.inputs.append(input_ids)\n",
    "\n",
    "        if self.training and sort:\n",
    "            self.indices = np.argsort(seq_lens)\n",
    "            self.inputs = [self.inputs[i] for i in self.indices]\n",
    "            self.labels = self.labels[self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Always drop last\n",
    "class BatchIterator(object):\n",
    "\n",
    "    def __init__(self, dataset, collate_fn, batch_size,\n",
    "                 shuffle=True, drop_last=True):\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(dataset)\n",
    "        self.shuffle = shuffle\n",
    "        if drop_last:\n",
    "            self.num_batches = self.size // batch_size\n",
    "        else:\n",
    "            self.num_batches = (self.size + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.random.choice(self.num_batches, self.num_batches, replace=False)\n",
    "        else:\n",
    "            indices = range(self.num_batches)\n",
    "        for idx in indices:\n",
    "            left = self.batch_size * idx\n",
    "            yield(self.collate_fn(self.dataset[left: left + self.batch_size]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xy_batch = [pad_sequence(batch[0], batch_first=True), batch[1]]\n",
    "    return xy_batch\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, valloader, optimizer, scheduler, val_ans, callbacks=None,\n",
    "              criterion=nn.BCEWithLogitsLoss(), verbose_step=10000):\n",
    "    t1 = time.time()\n",
    "    tr_loss = 0\n",
    "    tr_roc_auc = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs[:, 0], y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        tr_roc_auc += roc_auc_score(y_batch.cpu(), torch.sigmoid(outputs).data.cpu().numpy())\n",
    "        if callbacks is not None:\n",
    "            for func in callbacks:\n",
    "                func.on_batch_end(model)\n",
    "        if (step + 1) % verbose_step == 0:\n",
    "            loss_now = tr_loss / (step + 1)\n",
    "            print(f'step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s')\n",
    "    model.eval()\n",
    "    val_preds = model.predict(valloader)\n",
    "    val_ans = torch.tensor(val_ans.values, dtype=torch.float64)\n",
    "    val_loss = criterion(val_preds[:, 0], val_ans).item()\n",
    "    val_roc_auc = roc_auc_score(val_ans.int().cpu(), torch.sigmoid(val_preds).data.cpu().numpy())\n",
    "    print(f'val loss:{val_loss:.7f}')\n",
    "    if callbacks is not None:\n",
    "        for func in callbacks:\n",
    "            func.on_epoch_end(model)\n",
    "    experiment.log_metric(\"train loss\", tr_loss / (step + 1))\n",
    "    wandb.log({\"epoch train loss\": tr_loss / (step + 1)})\n",
    "    wandb.log({\"epoch val loss\": val_loss})\n",
    "    wandb.log({\"epoch train roc auc\": tr_roc_auc / (step + 1)})\n",
    "    wandb.log({\"epoch val roc auc\": val_roc_auc / (step + 1)})\n",
    "    scheduler.step(val_loss)\n",
    "    return tr_loss / (step + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns glove', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_wiki(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/working/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE) if len(o) > 100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns wiki', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_parag(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/working/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' '))\n",
    "                            for o in open(EMBEDDING_FILE, encoding='utf8', errors='ignore')\n",
    "                            if len(o) > 100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns parag', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def load_ggle(word_index, max_features, unk_uni):\n",
    "    EMBEDDING_FILE = data_path + 'embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "    embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "    embed_size = embeddings_index.get_vector('known').size\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in embeddings_index:\n",
    "                embedding_matrix[i] = embeddings_index.get_vector(word_lower)\n",
    "            else:\n",
    "                unknown_words.append((word, i))\n",
    "\n",
    "    print('\\nTotal unknowns ggle', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_all_embeddings(tokenizer, max_features, clean_num=False, unk_uni=True):\n",
    "    word_index = tokenizer.word_index\n",
    "    if clean_num == 2:\n",
    "        ggle_word_index = {}\n",
    "        for word, i in word_index.items():\n",
    "            ggle_word_index[clean_numbers(word)] = i\n",
    "    else:\n",
    "        ggle_word_index = word_index\n",
    "\n",
    "    embedding_matrix_1, u1 = load_glove(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_2, u2 = load_wiki(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_3, u3 = load_parag(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_4, u4 = load_ggle(ggle_word_index, max_features, unk_uni)\n",
    "    embedding_matrix = np.concatenate((embedding_matrix_1,\n",
    "                                       embedding_matrix_2,\n",
    "                                       embedding_matrix_3,\n",
    "                                       embedding_matrix_4), axis=1)\n",
    "    del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
    "    gc.collect()\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = ',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║\\\n",
    "―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n",
    "\n",
    "\n",
    "def clean_text(x, puncts=puncts): #добавляет пробелы вокруг пунктуации\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(train_df, val_df, test_df, max_len, max_features, trunc='pre',\n",
    "                 lower=False, clean_num=2, unk_uni=True):\n",
    "    train_df = train_df.copy()\n",
    "    train_y = train_df['target'].values\n",
    "\n",
    "    # lower\n",
    "    if lower:\n",
    "        train_df['question_text'] = train_df['question_text'].apply(lambda x: x.lower())\n",
    "        val_df['question_text'] = val_df['question_text'].apply(lambda x : x.lower())\n",
    "        test_df['question_text'] = test_df['question_text'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean the text\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    val_df['question_text'] = val_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "    # Clean numbers\n",
    "    if clean_num == 1:\n",
    "        train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_numbers(x))\n",
    "        val_df['question_text'] = val_df['question_text'].apply(lambda x: clean_numbers(x))\n",
    "        test_df['question_text'] = test_df['question_text'].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "    # fill up the missing values\n",
    "    train_df['question_text'] = train_df['question_text'].fillna('_##_')\n",
    "    val_df['question_text'] = val_df['question_text'].fillna('_##_')\n",
    "    test_df['question_text'] = test_df['question_text'].fillna('_##_')\n",
    "\n",
    "    train_X = train_df['question_text'].values\n",
    "    val_X = val_df['question_text'].values\n",
    "    test_X = test_df['question_text'].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n",
    "    tokenizer.fit_on_texts(train_X)\n",
    "    print(tokenizer)\n",
    "    print('len(vocab)', len(tokenizer.word_index))\n",
    "\n",
    "    train_X_ids = tokenizer.texts_to_sequences(train_X)\n",
    "    train_dataset = TextClassificationDataset(train_X_ids, train_y, sort=False)\n",
    "    train_loader = BatchIterator(train_dataset, collate_fn, batch_size)\n",
    "    \n",
    "    val_X_ids = tokenizer.texts_to_sequences(val_X)\n",
    "    val_dataset = TextClassificationDataset(val_X_ids, training=False)\n",
    "    val_loader = BatchIterator(val_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "    test_X_ids = tokenizer.texts_to_sequences(test_X)\n",
    "    test_dataset = TextClassificationDataset(test_X_ids, training=False)\n",
    "    test_loader = BatchIterator(test_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "    return train_loader, val_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path  + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df.copy(), train_size=0.7, random_state=seed)\n",
    "oneline = train_df[~train_df['question_text'].str.contains('\\n')]\n",
    "multiline = train_df[train_df['question_text'].str.contains('\\n')]\n",
    "oneline_size = len(oneline)\n",
    "train_idx, valid_idx = train_test_split(np.arange(oneline_size), train_size=0.9, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "augmented = pd.read_csv(\"../input/augmented-fairseq/augmented (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>When will men and boys learn the truth and go ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Why does Keralites (India) not speak Hindi?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>If you pay a $0 credit card and the company se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What are the real benefits of using Quora?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How much coffee is safe to drink?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      question_text  target\n",
       "0           0  When will men and boys learn the truth and go ...       1\n",
       "1           1        Why does Keralites (India) not speak Hindi?       0\n",
       "2           2  If you pay a $0 credit card and the company se...       0\n",
       "3           3         What are the real benefits of using Quora?       0\n",
       "4           4                  How much coffee is safe to drink?       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented['question_text'] = augmented['question_text'].str[2:-2]\n",
    "augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>487207</th>\n",
       "      <td>5f6988e5cb1096691086</td>\n",
       "      <td>When will men and boys learn the truth and go ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113645</th>\n",
       "      <td>da36c0a619e1cf3fcdd4</td>\n",
       "      <td>Why does not Keralites (India) does not like t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558619</th>\n",
       "      <td>6d73fed2c64a4883e27b</td>\n",
       "      <td>Will you still incur a cash advance fee if you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807057</th>\n",
       "      <td>9e22ba5c0a04d9a44236</td>\n",
       "      <td>What are the real advantages of using Quora?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163578</th>\n",
       "      <td>e4006d45becca4c20ab7</td>\n",
       "      <td>How much coffee is safe to drink?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "487207   5f6988e5cb1096691086   \n",
       "1113645  da36c0a619e1cf3fcdd4   \n",
       "558619   6d73fed2c64a4883e27b   \n",
       "807057   9e22ba5c0a04d9a44236   \n",
       "1163578  e4006d45becca4c20ab7   \n",
       "\n",
       "                                             question_text  target  \n",
       "487207   When will men and boys learn the truth and go ...       1  \n",
       "1113645  Why does not Keralites (India) does not like t...       0  \n",
       "558619   Will you still incur a cash advance fee if you...       0  \n",
       "807057        What are the real advantages of using Quora?       0  \n",
       "1163578                  How much coffee is safe to drink?       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = pd.DataFrame({\n",
    "    'question_text' : \n",
    "    list(oneline.iloc[train_idx]['question_text'].values) +\n",
    "    #list(augmented.iloc[train_idx]['question_text'].values) +\n",
    "    list(multiline['question_text'].values),\n",
    "    'target' :\n",
    "    list(oneline.iloc[train_idx]['target'].values) +\n",
    "    #list(augmented.iloc[train_idx]['target'].values) +\n",
    "    list(multiline['target'].values)\n",
    "})\n",
    "\n",
    "valid_df = oneline.iloc[valid_idx]\n",
    "\n",
    "train_aug.drop_duplicates(inplace=True)\n",
    "train_aug = train_aug.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914285 822857\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.index),  len(train_aug.drop_duplicates().index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del augmented\n",
    "del train_df\n",
    "del oneline\n",
    "del multiline\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  (822857, 2)\n",
      "Validition :  (91428, 3)\n",
      "Test:  (391837, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train : ', train_aug.shape)\n",
    "print('Validition : ', valid_df.shape)\n",
    "print('Test: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7f3174ceaf28>\n",
      "len(vocab) 153456\n"
     ]
    }
   ],
   "source": [
    "data_path = '/kaggle/working/'\n",
    "obj = prepare_data(train_aug, valid_df, test_df, max_len, max_features,\n",
    "                   trunc=trunc, lower=lower, clean_num=clean_num, unk_uni=unk_uni)\n",
    "\n",
    "train_loader, val_loader, test_loader, tokenizer = obj\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unknowns glove 31099\n",
      "[('polinomials', 119984), ('geniecashbox', 119985), ('ambazari', 119988), ('noodliness', 119989), ('allahabaad', 119992), ('urinalals', 119993), ('magnificia', 119994), ('disabilitites', 119995), ('1023m', 119998), ('08976m', 119999)]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_1, _ = load_glove(word_index, max_features, unk_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /kaggle/input/quora-insincere-questions-classification/embeddings.zip\r\n",
      "   creating: /kaggle/working/embeddings/wiki-news-300d-1M/\r\n",
      "  inflating: /kaggle/working/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec  \r\n",
      "\n",
      "Total unknowns wiki 47473\n",
      "[('geniecashbox', 119985), ('ambazari', 119988), ('noodliness', 119989), ('uriah', 119990), ('allahabaad', 119992), ('urinalals', 119993), ('magnificia', 119994), ('disabilitites', 119995), ('1023m', 119998), ('08976m', 119999)]\n"
     ]
    }
   ],
   "source": [
    "!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'wiki-news-300d-1M/*' -d /kaggle/working/embeddings/\n",
    "embedding_matrix_2, _ = load_wiki(word_index, max_features, unk_uni)\n",
    "!rm -r /kaggle/working/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'paragram_300_sl999/*' -d /kaggle/working/embeddings/\\nembedding_matrix_3, _ = load_parag(word_index, max_features, unk_uni)\\n!rm -r /kaggle/working/embeddings\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'paragram_300_sl999/*' -d /kaggle/working/embeddings/\n",
    "embedding_matrix_3, _ = load_parag(word_index, max_features, unk_uni)\n",
    "!rm -r /kaggle/working/embeddings\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'GoogleNews-vectors-negative300/*' -d /kaggle/working/embeddings/\\nembedding_matrix_4, _ = load_ggle(word_index, max_features, unk_uni)\\n!rm -r /kaggle/working/embeddings\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'GoogleNews-vectors-negative300/*' -d /kaggle/working/embeddings/\n",
    "embedding_matrix_4, _ = load_ggle(word_index, max_features, unk_uni)\n",
    "!rm -r /kaggle/working/embeddings\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate((embedding_matrix_1,\n",
    "                                   embedding_matrix_2),\n",
    "                                   #embedding_matrix_3,\n",
    "                                   #embedding_matrix_4), \n",
    "                                   axis=1)\n",
    "del embedding_matrix_2,embedding_matrix_1#, embedding_matrix_4, embedding_matrix_3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.Tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ans, test_ans = valid_df['target'], test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_s = [list(range(300)), list(range(300, 600)),\n",
    "         list(range(600, 900)), list(range(900, 1200))]\n",
    "\n",
    "cols_s = [ids_s[0] + ids_s[1],\n",
    "          ids_s[0] + ids_s[2],\n",
    "          ids_s[1] + ids_s[2],\n",
    "          ids_s[1] + ids_s[3],\n",
    "          ids_s[2] + ids_s[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop): Dropout(p=0.2, inplace=False)\n",
      "  (proj): Linear(in_features=600, out_features=128, bias=True)\n",
      "  (proj_act): ReLU()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 341121\n",
      "val loss:0.1059478\n",
      "val loss:0.1012606\n",
      "val loss:0.0998079\n",
      "val loss:0.0980836\n",
      "val loss:0.1001801\n",
      "val loss:0.0973511\n",
      "n_model:1 33.4s/epoch\n",
      "n_model:1 0.9679676052648442\n",
      "9.4s\n",
      "0.35 0.29687499999999983\n",
      "val f1:  0.6867242087957254\n",
      "test f1:  0.6829915236319604\n"
     ]
    }
   ],
   "source": [
    "val_probas = np.zeros((len(valid_df), 1))\n",
    "test_probas = np.zeros((len(test_df), 1))\n",
    "\n",
    "model = GRUModel(n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim,\n",
    "                 pretrained_embedding=embedding_matrix,\n",
    "                 fix_embedding=fix_embedding, padding_idx=0)\n",
    "print(model)\n",
    "print('#Trainable params', get_param_size(model))\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=2, factor=0.5)\n",
    "t2 = time.time()\n",
    "wandb.watch(model)\n",
    "model.fit(train_loader, val_loader, aug_epochs, optimizer, scheduler, val_ans)\n",
    "print(f'n_model:{1} {(time.time() - t2) / (aug_epochs):.1f}s/epoch')\n",
    "t3 = time.time()\n",
    "val_probas =  model.predict_proba(val_loader)\n",
    "test_probas = model.predict_proba(test_loader)\n",
    "print(f'n_model:{1} {roc_auc_score(val_ans, val_probas)}')\n",
    "print(f'{time.time() - t3:.1f}s')\n",
    "res = scipy.optimize.minimize(\n",
    "    lambda t: -f1_score(val_ans, (val_probas >= t).astype(np.int)),\n",
    "    x0=0.5,\n",
    "    method='Nelder-Mead',\n",
    "    tol=1e-3,\n",
    ")\n",
    "best_threshold = res.x[0]\n",
    "print(dropout, best_threshold)\n",
    "print(\"val f1: \", f1_score(val_ans, (val_probas >= best_threshold)))\n",
    "print(\"test f1: \", f1_score(test_ans, (test_probas >= best_threshold)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29687499999999983\n"
     ]
    }
   ],
   "source": [
    "import scipy \n",
    "\n",
    "res = scipy.optimize.minimize(\n",
    "    lambda t: -f1_score(val_ans, (val_probas >= t).astype(np.int)),\n",
    "    x0=0.5,\n",
    "    method='Nelder-Mead',\n",
    "    tol=1e-3,\n",
    ")\n",
    "best_threshold = res.x[0]\n",
    "print(best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6867242087957254\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(val_ans, (val_probas >= best_threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1-score: 0.6829915236319604\n",
      "test roc-auc score: 0.9673263311253577\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(test_ans, test_probas >= best_threshold)\n",
    "roc_auc = roc_auc_score(test_ans, test_probas)\n",
    "print('test f1-score: {}'.format(f1))\n",
    "print('test roc-auc score: {}'.format(roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
