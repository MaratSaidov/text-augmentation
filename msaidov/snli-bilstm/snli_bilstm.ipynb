{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "snli-bilstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WdIFqUXnPvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYV7fZJlseX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!pip install catalyst\n",
        "!pip install alchemy-catalyst\n",
        "!wandb login c54b2fcb6b8ca2808f5be303a8a3b6e464f52cca\n",
        "\n",
        "is_alchemy_used = True\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qQpeBNZt3SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "wandb.init(project=\"text-augmentation\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bchgg4iZ_5um",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zaJUn08ji3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import pdb\n",
        "from torch import cuda\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.data import BPTTIterator, BucketIterator, Iterator\n",
        "from torchtext import datasets\n",
        "import torch.optim as O\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "from catalyst.dl import SupervisedRunner\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HWbKmUyykgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_device():\n",
        "\tif torch.cuda.is_available():\n",
        "\t\treturn torch.device('cuda:0')\n",
        "\telse:\n",
        "\t\treturn torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJUg6ODp_PUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = get_device()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0xbXykn9j8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_STACKING_NUMBER = 3\n",
        "FULLY_CONNECTED_DIM = 2048\n",
        "LSTM_INPUT_SIZE = 300\n",
        "EMBEDDING_DIMENSION = 300\n",
        "HIDDEN_DIMENSION = 512\n",
        "OUTPUT_DIMENSION = 3\n",
        "BATCH_SIZE = 128\n",
        "MODEL_PATH = 'drive/My Drive/text-augmentation/log-directory/baseline-github.pt'\n",
        "LOG_DIRECTORY = 'drive/My Drive/text-augmentation/log-directory/logs/'\n",
        "LR_STEP = 0.001\n",
        "EPOCHS_NUMBER = 5\n",
        "DP_RATIO = 0.2\n",
        "LOG_INTERVAL = 50\n",
        "BPTT_LEN = 5\n",
        "MINI_BATCH = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JiRRQXGtZzkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\tdef __init__(self, vocab_size, dp_ratio=DP_RATIO,\n",
        "\t             embed_dim=EMBEDDING_DIMENSION, hidden_dim=HIDDEN_DIMENSION,\n",
        "\t             lstm_input_size=LSTM_INPUT_SIZE, fully_connected_dim=FULLY_CONNECTED_DIM):\n",
        "\t\tsuper(BiLSTM, self).__init__()\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embed_dim = embed_dim\n",
        "\t\tself.dp_ratio = dp_ratio\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\tself.lstm_input_size = lstm_input_size\n",
        "\t\tself.linear_input_size = 8 * self.hidden_dim\n",
        "\t\tself.fully_connected_dim = fully_connected_dim\n",
        "\n",
        "\t\tself.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "\t\tself.projection = nn.Linear(self.embed_dim, self.lstm_input_size)\n",
        "\t\tself.lstm = nn.LSTM(self.lstm_input_size, self.hidden_dim,\n",
        "\t\t                    LSTM_STACKING_NUMBER, bidirectional=True)\n",
        "\t\tself.out = nn.Sequential(\n",
        "\t\t\tnn.Linear(self.linear_input_size, self.fully_connected_dim),\n",
        "\t\t\tnn.Dropout(p=self.dp_ratio),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(self.fully_connected_dim, self.fully_connected_dim),\n",
        "\t\t\tnn.Dropout(p=self.dp_ratio),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(self.fully_connected_dim, self.fully_connected_dim),\n",
        "\t\t\tnn.Dropout(p=self.dp_ratio),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(self.fully_connected_dim, OUTPUT_DIMENSION)\n",
        "\t\t)\n",
        "\t\tpass\n",
        "\n",
        "\tdef forward(self, batch):\n",
        "\t\tpremise_embed = self.embedding(batch[0])\n",
        "\t\thypothesis_embed = self.embedding(batch[1])\n",
        "\t\tpremise_proj = F.relu(self.projection(premise_embed))\n",
        "\t\thypothesis_proj = F.relu(self.projection(hypothesis_embed))\n",
        "\t\tencoded_premise, (h0_premise, c0_premise) = self.lstm(premise_proj)\n",
        "\t\tencoded_hypothesis, (h0_hypothesis, c0_hypothesis) = self.lstm(hypothesis_proj)\n",
        "\n",
        "\t\th0_premise = h0_premise[-1, :, :]\n",
        "\t\tc0_premise = c0_premise[-1, :, :]\n",
        "\t\th0_hypothesis = h0_hypothesis[-1, :, :]\n",
        "\t\tc0_hypothesis = c0_hypothesis[-1, :, :]\n",
        "\t\th0_premise.unsqueeze_(-1)\n",
        "\t\tc0_premise.unsqueeze_(-1)\n",
        "\t\th0_hypothesis.unsqueeze_(-1)\n",
        "\t\tc0_hypothesis.unsqueeze_(-1)\n",
        "\t\th0_premise = h0_premise.expand(-1, -1, BATCH_SIZE)\n",
        "\t\tc0_premise = c0_premise.expand(-1, -1, BATCH_SIZE)\n",
        "\t\th0_hypothesis = h0_hypothesis.expand(-1, -1, BATCH_SIZE)\n",
        "\t\tc0_hypothesis = c0_hypothesis.expand(-1, -1, BATCH_SIZE)\n",
        "  \n",
        "\t\th0_premise = h0_premise.transpose(2, 0)\n",
        "\t\tc0_premise = c0_premise.transpose(2, 0)\n",
        "\t\th0_premise = h0_premise.transpose(2, 1)\n",
        "\t\tc0_premise = c0_premise.transpose(2, 1)\n",
        "\t\th0_hypothesis = h0_hypothesis.transpose(2, 0)\n",
        "\t\tc0_hypothesis = c0_hypothesis.transpose(2, 0)\n",
        "\t\th0_hypothesis = h0_hypothesis.transpose(2, 1)\n",
        "\t\tc0_hypothesis = c0_hypothesis.transpose(2, 1)\n",
        "\n",
        "\t\th0_premise = h0_premise.mean(1)\n",
        "\t\tc0_premise = c0_premise.mean(1)\n",
        "\t\th0_hypothesis = h0_hypothesis.mean(1)\n",
        "\t\tc0_hypothesis = c0_hypothesis.mean(1)\n",
        "\t\tencoded_premise = encoded_premise.mean(1)\n",
        "\t\tencoded_hypothesis = encoded_hypothesis.mean(1)\n",
        "\n",
        "\t\tlstm_premise = torch.cat((h0_premise, c0_premise), 1)\n",
        "\t\tlstm_hypothesis = torch.cat((h0_hypothesis, c0_hypothesis), 1)\n",
        "\n",
        "\t\tlstm_premise = lstm_premise.narrow(0, 0, encoded_premise.shape[0])\n",
        "\t\tlstm_hypothesis = lstm_hypothesis.narrow(0, 0, encoded_hypothesis.shape[0])\n",
        "\n",
        "\t\tpremise = torch.cat((encoded_premise, lstm_premise), 1)\n",
        "\t\thypothesis = torch.cat((encoded_hypothesis, lstm_hypothesis), 1)\n",
        "  \n",
        "\t\tcombined = torch.cat((premise, hypothesis), 1)\n",
        "\t\treturn self.out(combined)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85bYnq1I7R2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = data.Field(lower=True, tokenize='spacy', batch_first=True)\n",
        "answers = data.LabelField(sequential=False, unk_token=None, is_target=True)\n",
        "\n",
        "train, validate, test = datasets.SNLI.splits(inputs, answers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6JyxwCqH-XS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.examples = train.examples[:len(train.examples) // 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VLRf8HHaZzk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs.build_vocab(train, validate)\n",
        "answers.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi2qkS0GvNUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, validate_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train, validate, test), batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otL4yxB8vO7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(inputs.vocab)\n",
        "out_dim = len(answers.vocab)\n",
        "labels = answers.vocab.stoi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6V6A0GsUuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized__ = False\n",
        "\n",
        "    def __init__(self, iterator: Iterator):\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = 1\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized__ = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(lambda batch: {\n",
        "                    'features': (batch.premise, batch.hypothesis),\n",
        "                    'targets': batch.label,\n",
        "                }, self.batch_sampler.__iter__())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewl6MjSYtIuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = Iterator(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_iter = Iterator(validate, batch_size=BATCH_SIZE)\n",
        "\n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        " \n",
        "loaders = {'train': train_iter, 'valid': valid_iter}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "buk-3rBXZzlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainAndValidate():\n",
        "    def __init__(self, batch_size=BATCH_SIZE, embed_dim=EMBEDDING_DIMENSION,\n",
        "                 hidden_dim=HIDDEN_DIMENSION, dp_ratio=DP_RATIO, epochs=EPOCHS_NUMBER,\n",
        "                 lr=LR_STEP, save_path=MODEL_PATH):\n",
        "        print(\"Training process has begun at: {}\".format(datetime.datetime.now()))\n",
        "        self.batch_size = batch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dp_ratio = dp_ratio\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.save_path = save_path\n",
        "        self.log_interval = LOG_INTERVAL\n",
        "\n",
        "        self.model = BiLSTM(vocab_size)\n",
        "        self.model.to(device)\n",
        "        wandb.watch(self.model)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "        self.optimizer = O.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer)\n",
        "        self.best_accuracy = -1\n",
        "        print(\"Resource preparation done: {}\".format(datetime.datetime.now()))\n",
        "\n",
        "    def save_model(self, current_accuracy):\n",
        "        if current_accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = current_accuracy\n",
        "            model_options = {'vocab_size' : vocab_size,\n",
        "                             'embed_dim' : EMBEDDING_DIMENSION,\n",
        "                             'dp_ratio' : DP_RATIO, \n",
        "                             'hidden_dim' : HIDDEN_DIMENSION,\n",
        "                             'out_dim': OUTPUT_DIMENSION\n",
        "                            }\n",
        "            torch.save({\n",
        "                'accuracy': self.best_accuracy,\n",
        "                'options': self.model_options,\n",
        "                'model_dict': self.model.state_dict(),\n",
        "                }, self.save_path)\n",
        "        return\n",
        "\n",
        "    def execute(self):\n",
        "        n_correct, n_total, n_loss = 0, 0, 0\n",
        "        print(f\"Number of iterations: {len(train_iterator)}\")\n",
        "        runner = SupervisedRunner()\n",
        "        runner.train(model=self.model, criterion=self.criterion, optimizer=self.optimizer,\n",
        "                    scheduler=self.scheduler, loaders=loaders, logdir=LOG_DIRECTORY,\n",
        "                    num_epochs=EPOCHS_NUMBER, verbose=True)\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo1CINzW-IkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = TrainAndValidate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajJOI0amO7ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir 'drive/My Drive/text-augmentation/log-directory/logs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcesaBLFOh8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task.execute()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suN8RHCcMqlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save({'model_dict': task.model.state_dict()}, 'drive/My Drive/text-augmentation/github-baseline.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLqDXEs5RAza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}