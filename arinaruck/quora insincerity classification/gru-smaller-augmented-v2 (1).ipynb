{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting comet_ml\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/c6/fac88f43f2aa61a09fee4ffb769c73fe93fe7de75764246e70967d31da09/comet_ml-3.0.2-py3-none-any.whl (170kB)\r\n",
      "\u001b[K     |████████████████████████████████| 174kB 2.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from comet_ml) (1.13.0)\r\n",
      "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\r\n",
      "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\r\n",
      "Collecting netifaces>=0.10.7\r\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (2.22.0)\r\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (2.0.0)\r\n",
      "Collecting jsonschema<3.1.0,>=2.6.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/48/f5f11003ceddcd4ad292d4d9b5677588e9169eef41f88e38b2888e7ec6c4/jsonschema-3.0.2-py2.py3-none-any.whl (54kB)\r\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \r\n",
      "\u001b[?25hCollecting comet-git-pure>=0.19.11\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/91/b191ae375380332f82aaa83a41c45844ee1809198085cd267fbcb95cce86/comet_git_pure-0.19.14-py3-none-any.whl (401kB)\r\n",
      "\u001b[K     |████████████████████████████████| 409kB 43.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: websocket-client>=0.55.0 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (0.56.0)\r\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.6/site-packages (from comet_ml) (7.352.0)\r\n",
      "Collecting configobj; extra == \"ini\"\r\n",
      "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (1.24.2)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (2019.9.11)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18.4->comet_ml) (3.0.4)\r\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->comet_ml) (0.15.6)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->comet_ml) (19.3.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->comet_ml) (42.0.1.post20191125)\r\n",
      "Building wheels for collected packages: configobj\r\n",
      "  Building wheel for configobj (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34546 sha256=db5c2299ca7d83f8fc7e242778be984895565157ae21d18db600d96f8acc0151\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\r\n",
      "Successfully built configobj\r\n",
      "Installing collected packages: configobj, everett, netifaces, jsonschema, comet-git-pure, comet-ml\r\n",
      "  Found existing installation: jsonschema 3.2.0\r\n",
      "    Uninstalling jsonschema-3.2.0:\r\n",
      "      Successfully uninstalled jsonschema-3.2.0\r\n",
      "Successfully installed comet-git-pure-0.19.14 comet-ml-3.0.2 configobj-5.0.6 everett-1.0.2 jsonschema-3.0.2 netifaces-0.10.9\r\n"
     ]
    }
   ],
   "source": [
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/paragram-300-sl999/paragram_300_sl999.txt\n",
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/augmented/augmented.csv\n",
      "/kaggle/input/augmentation/augmentation.csv\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/quora-insincere-questions-classification/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "data_path = '/kaggle/input/quora-insincere-questions-classification/'\n",
    "seed = 2077\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/__notebook__.ipynb\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/comet-ml-testing/gru-classificaton/3514da489a6a4dd59874c116121334fa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api = 'GGF21Vtrnid3Cgat9n1nL9Vcc'\n",
    "experiment = Experiment(api_key=api, project_name=\"GRU classificaton\", workspace=\"comet-ml testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "max_len = 50\n",
    "lower = True\n",
    "trunc = 'pre'\n",
    "max_features = 120000\n",
    "n_vocab = max_features\n",
    "clean_num = 0\n",
    "\n",
    "# Training\n",
    "#n_models = 6\n",
    "n_models = 5\n",
    "aug_epochs = 8\n",
    "batch_size = 512\n",
    "drop_last = True\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "# Embedding\n",
    "fix_embedding = True\n",
    "unk_uni = True  # Initializer for unknown words делает вектор для незнакомого слова из N(emb.mean(), emb.std())\n",
    "n_embed = 2\n",
    "embed_dim = n_embed * 300 \n",
    "proj_dim = hidden_dim\n",
    "\n",
    "# GRU\n",
    "bidirectional = True\n",
    "n_layers = 1\n",
    "rnn_dim = hidden_dim\n",
    "\n",
    "# The second last Linear layer\n",
    "dense_dim = 2 * rnn_dim if bidirectional else rnn_dim\n",
    "\n",
    "# EMA\n",
    "mu = 0.9\n",
    "#mu = 0\n",
    "updates_per_epoch = 10\n",
    "\n",
    "# Test set\n",
    "threshold = 0.353\n",
    "test_batch_size = 8 * batch_size\n",
    "test_batch_size = batch_size\n",
    "\n",
    "def seed_torch(seed=1):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_param_size(model, trainable=True):\n",
    "    if trainable:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n",
    "    else:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    return psize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-apply-exponential-moving-average-decay-for-variables/10856\n",
    "class EMA():\n",
    "    def __init__(self, model, mu, level='batch', n=1):\n",
    "        \"\"\"\n",
    "        level: 'batch' or 'epoch'\n",
    "          'batch': Update params every n batches.\n",
    "          'epoch': Update params every epoch.\n",
    "        \"\"\"\n",
    "        # self.ema_model = copy.deepcopy(model)\n",
    "        self.mu = mu\n",
    "        self.level = level\n",
    "        self.n = n\n",
    "        self.cnt = self.n\n",
    "        self.shadow = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data\n",
    "\n",
    "    def _update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1 - self.mu) * param.data + self.mu * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def set_weights(self, ema_model):\n",
    "        for name, param in ema_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def on_batch_end(self, model):\n",
    "        if self.level is 'batch':\n",
    "            self.cnt -= 1\n",
    "            if self.cnt == 0:\n",
    "                self._update(model)\n",
    "                self.cnt = self.n\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        if self.level is 'epoch':\n",
    "            self._update(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z, _ = torch.max(inputs, 1)\n",
    "        return z\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim,\n",
    "                 padding_idx=0, pretrained_embedding=None, fix_embedding=True,\n",
    "                 n_out=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fix_embedding = fix_embedding\n",
    "        self.padding_idx = padding_idx\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(pretrained_embedding, freeze=fix_embedding)\n",
    "            self.embed.padding_idx = self.padding_idx\n",
    "        else:\n",
    "            self.embed = nn.Embedding(self.n_vocab, self.embed_dim, padding_idx=self.padding_idx)\n",
    "        self.proj = nn.Linear(embed_dim, proj_dim)\n",
    "        self.proj_act = nn.ReLU()\n",
    "        self.gru = nn.GRU(proj_dim, rnn_dim, self.n_layers,\n",
    "                          batch_first=True, bidirectional=bidirectional)\n",
    "        self.pooling = GlobalMaxPooling1D()\n",
    "        in_dim = 2 * rnn_dim if self.bidirectional else rnn_dim\n",
    "        self.dense = nn.Linear(in_dim, dense_dim)\n",
    "        self.dense_act = nn.ReLU()\n",
    "        self.out_linear = nn.Linear(dense_dim, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (bs, max_len)\n",
    "        x = self.embed(inputs)\n",
    "        x = self.proj_act(self.proj(x))\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dense_act(self.dense(x))\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, dataloader, epochs, optimizer, callbacks=None):\n",
    "        for i in range(epochs):\n",
    "            run_epoch(model, dataloader, optimizer, callbacks=callbacks)\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                X_batch = batch[0]\n",
    "                preds.append(self.forward(X_batch).data.cpu())\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def predict_proba(self, dataloader):\n",
    "        return torch.sigmoid(self.predict(dataloader)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    token_ids_s : List of token ids\n",
    "    labels   : Target labels\n",
    "    training: Sort token_ids_s by length if training is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_ids_s, labels=None, max_len=1000, training=True, sort=True):\n",
    "        self.training = training\n",
    "\n",
    "        if labels is None:\n",
    "            self.labels = torch.ones(len(token_ids_s), dtype=torch.long)  # dummy\n",
    "        else:\n",
    "            self.labels = torch.LongTensor(labels)\n",
    "\n",
    "        seq_lens = []\n",
    "        self.inputs = []\n",
    "        for e, token_ids in enumerate(token_ids_s):\n",
    "            seq_lens.append(len(token_ids))\n",
    "            input_ids = torch.LongTensor(token_ids[:max_len])\n",
    "            self.inputs.append(input_ids)\n",
    "\n",
    "        if self.training and sort:\n",
    "            self.indices = np.argsort(seq_lens)\n",
    "            self.inputs = [self.inputs[i] for i in self.indices]\n",
    "            self.labels = self.labels[self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Always drop last\n",
    "class BatchIterator(object):\n",
    "\n",
    "    def __init__(self, dataset, collate_fn, batch_size,\n",
    "                 shuffle=True, drop_last=True):\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(dataset)\n",
    "        self.shuffle = shuffle\n",
    "        if drop_last:\n",
    "            self.num_batches = self.size // batch_size\n",
    "        else:\n",
    "            self.num_batches = (self.size + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.random.choice(self.num_batches, self.num_batches, replace=False)\n",
    "        else:\n",
    "            indices = range(self.num_batches)\n",
    "        for idx in indices:\n",
    "            left = self.batch_size * idx\n",
    "            yield(self.collate_fn(self.dataset[left: left + self.batch_size]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xy_batch = [pad_sequence(batch[0], batch_first=True), batch[1]]\n",
    "    return xy_batch\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, optimizer, callbacks=None,\n",
    "              criterion=nn.BCEWithLogitsLoss(), verbose_step=10000):\n",
    "    t1 = time.time()\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs[:, 0], y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        if callbacks is not None:\n",
    "            for func in callbacks:\n",
    "                func.on_batch_end(model)\n",
    "        if (step + 1) % verbose_step == 0:\n",
    "            loss_now = tr_loss / (step + 1)\n",
    "            print(f'step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s')\n",
    "    if callbacks is not None:\n",
    "        for func in callbacks:\n",
    "            func.on_epoch_end(model)\n",
    "    experiment.log_metric(\"train loss\", tr_loss / (step + 1))\n",
    "    return tr_loss / (step + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns glove', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_wiki(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/working/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(EMBEDDING_FILE) if len(o) > 100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns wiki', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_parag(word_index, max_features, unk_uni):\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    EMBEDDING_FILE = '/kaggle/working/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(' '))\n",
    "                            for o in open(EMBEDDING_FILE, encoding='utf8', errors='ignore')\n",
    "                            if len(o) > 100)\n",
    "\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "            if embedding_vector is None:\n",
    "                unknown_words.append((word, i))\n",
    "            else:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('\\nTotal unknowns parag', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def load_ggle(word_index, max_features, unk_uni):\n",
    "    EMBEDDING_FILE = data_path + 'embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "    embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "    embed_size = embeddings_index.get_vector('known').size\n",
    "\n",
    "    unknown_words = []\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    if unk_uni:\n",
    "        embedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in embeddings_index:\n",
    "                embedding_matrix[i] = embeddings_index.get_vector(word_lower)\n",
    "            else:\n",
    "                unknown_words.append((word, i))\n",
    "\n",
    "    print('\\nTotal unknowns ggle', len(unknown_words))\n",
    "    print(unknown_words[-10:])\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_all_embeddings(tokenizer, max_features, clean_num=False, unk_uni=True):\n",
    "    word_index = tokenizer.word_index\n",
    "    if clean_num == 2:\n",
    "        ggle_word_index = {}\n",
    "        for word, i in word_index.items():\n",
    "            ggle_word_index[clean_numbers(word)] = i\n",
    "    else:\n",
    "        ggle_word_index = word_index\n",
    "\n",
    "    embedding_matrix_1, u1 = load_glove(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_2, u2 = load_wiki(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_3, u3 = load_parag(word_index, max_features, unk_uni)\n",
    "    embedding_matrix_4, u4 = load_ggle(ggle_word_index, max_features, unk_uni)\n",
    "    embedding_matrix = np.concatenate((embedding_matrix_1,\n",
    "                                       embedding_matrix_2,\n",
    "                                       embedding_matrix_3,\n",
    "                                       embedding_matrix_4), axis=1)\n",
    "    del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
    "    gc.collect()\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def setup_emb(tr_X, max_features=50000, clean_num=2, unk_uni=True):\n",
    "    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n",
    "    tokenizer.fit_on_texts(tr_X)\n",
    "    print(tokenizer)\n",
    "    print('len(vocab)', len(tokenizer.word_index))\n",
    "    embedding_matrix = load_all_embeddings(tokenizer, max_features=max_features,\n",
    "                                           clean_num=clean_num, unk_uni=unk_uni)\n",
    "    # np.save(embed_path, embedding_matrix)\n",
    "    return tokenizer, embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = ',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║\\\n",
    "―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n",
    "\n",
    "\n",
    "def clean_text(x, puncts=puncts): #добавляет пробелы вокруг пунктуации\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(train_df, test_df, max_len, max_features, trunc='pre',\n",
    "                 lower=False, clean_num=2, unk_uni=True):\n",
    "    train_df = train_df.copy()\n",
    "    train_y = train_df['target'].values\n",
    "\n",
    "    # lower\n",
    "    if lower:\n",
    "        train_df['question_text'] = train_df['question_text'].apply(lambda x: x.lower())\n",
    "        test_df['question_text'] = test_df['question_text'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean the text\n",
    "    train_df['question_text'] = train_df['question_text'].apply(\n",
    "        lambda x: clean_text(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(\n",
    "        lambda x: clean_text(x))\n",
    "\n",
    "    # Clean numbers\n",
    "    if clean_num == 1:\n",
    "        train_df['question_text'] = train_df['question_text'].apply(\n",
    "            lambda x: clean_numbers(x))\n",
    "        test_df['question_text'] = test_df['question_text'].apply(\n",
    "            lambda x: clean_numbers(x))\n",
    "\n",
    "    # fill up the missing values\n",
    "    train_df['question_text'] = train_df['question_text'].fillna('_##_')\n",
    "    test_df['question_text'] = test_df['question_text'].fillna('_##_')\n",
    "\n",
    "    train_X = train_df['question_text'].values\n",
    "    test_X = test_df['question_text'].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n",
    "    tokenizer.fit_on_texts(train_X)\n",
    "    print(tokenizer)\n",
    "    print('len(vocab)', len(tokenizer.word_index))\n",
    "\n",
    "    train_X_ids = tokenizer.texts_to_sequences(train_X)\n",
    "    train_dataset = TextClassificationDataset(train_X_ids, train_y, sort=False)\n",
    "    train_loader = BatchIterator(train_dataset, collate_fn, batch_size)\n",
    "\n",
    "    test_X_ids = tokenizer.texts_to_sequences(test_X)\n",
    "    test_dataset = TextClassificationDataset(test_X_ids, training=False)\n",
    "    test_loader = BatchIterator(test_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "    return train_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_s = [list(range(300)), list(range(300, 600)),\n",
    "         list(range(600, 900)), list(range(900, 1200))]\n",
    "\n",
    "cols_s = [ids_s[0] + ids_s[1],\n",
    "          ids_s[0] + ids_s[2],\n",
    "          ids_s[1] + ids_s[2],\n",
    "          ids_s[1] + ids_s[3],\n",
    "          ids_s[2] + ids_s[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path  + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df.copy(), train_size=0.7, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>487207</th>\n",
       "      <td>5f6988e5cb1096691086</td>\n",
       "      <td>When will men and boys learn the truth and go ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113645</th>\n",
       "      <td>da36c0a619e1cf3fcdd4</td>\n",
       "      <td>Why does not Keralites (India) does not like t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558619</th>\n",
       "      <td>6d73fed2c64a4883e27b</td>\n",
       "      <td>Will you still incur a cash advance fee if you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807057</th>\n",
       "      <td>9e22ba5c0a04d9a44236</td>\n",
       "      <td>What are the real advantages of using Quora?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163578</th>\n",
       "      <td>e4006d45becca4c20ab7</td>\n",
       "      <td>How much coffee is safe to drink?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "487207   5f6988e5cb1096691086   \n",
       "1113645  da36c0a619e1cf3fcdd4   \n",
       "558619   6d73fed2c64a4883e27b   \n",
       "807057   9e22ba5c0a04d9a44236   \n",
       "1163578  e4006d45becca4c20ab7   \n",
       "\n",
       "                                             question_text  target  \n",
       "487207   When will men and boys learn the truth and go ...       1  \n",
       "1113645  Why does not Keralites (India) does not like t...       0  \n",
       "558619   Will you still incur a cash advance fee if you...       0  \n",
       "807057        What are the real advantages of using Quora?       0  \n",
       "1163578                  How much coffee is safe to drink?       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "augmented = pd.read_csv(\"../input/augmented/augmented.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['question_text'] = augmented['question_text'].str[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneliners = int(len(augmented) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685710.0\n"
     ]
    }
   ],
   "source": [
    "print(0.75 * oneliners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>when are men and boys going to learn the truth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>why does keralites ( india not ) ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>if you pay a card card for $ zero , and the co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what are the real benefits of using quora ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>how much coffee is safe to drink ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      question_text  target\n",
       "0           0  when are men and boys going to learn the truth...       1\n",
       "1           1                 why does keralites ( india not ) ?       0\n",
       "2           2  if you pay a card card for $ zero , and the co...       0\n",
       "3           3        what are the real benefits of using quora ?       0\n",
       "4           4                 how much coffee is safe to drink ?       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = pd.DataFrame({'question_text' : list(train_df['question_text'].values) + list(augmented['question_text'].values[:3*oneliners]),\n",
    "                               'target' : list(train_df['target'].values) + list(augmented['target'].values[:3*oneliners])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del augmented\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_df.index) / len(train_aug.drop_duplicates().index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  (3657125, 2)\n",
      "Validition :  (391837, 3)\n"
     ]
    }
   ],
   "source": [
    "val_ans = val_df['target']\n",
    "\n",
    "print('Train : ', train_aug.shape)\n",
    "print('Validition : ', val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7f8b164ed978>\n",
      "len(vocab) 342951\n"
     ]
    }
   ],
   "source": [
    "data_path = '/kaggle/working/'\n",
    "obj = prepare_data(train_aug, val_df[['question_text']], max_len, max_features,\n",
    "                   trunc=trunc, lower=lower, clean_num=clean_num, unk_uni=unk_uni)\n",
    "\n",
    "train_aug_loader, test_loader, tokenizer = obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unknowns glove 58329\n",
      "[('technipfmc', 119949), ('spectrul', 119964), ('saanvi', 119966), ('conjuct', 119976), ('shengen', 119979), ('quore', 119983), ('gopakumar', 119985), ('bohuslän', 119990), ('hurricains', 119994), ('padha', 119998)]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_1, _ = load_glove(word_index, max_features, unk_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /kaggle/input/quora-insincere-questions-classification/embeddings.zip\r\n",
      "   creating: /kaggle/working/embeddings/wiki-news-300d-1M/\r\n",
      "  inflating: /kaggle/working/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec  \r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'wiki-news-300d-1M/*' -d /kaggle/working/embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unknowns wiki 68700\n",
      "[('shengen', 119979), ('bhd', 119982), ('quore', 119983), ('gopakumar', 119985), ('pittbull', 119986), ('bohuslän', 119990), ('cussler', 119992), ('hurricains', 119994), ('roatan', 119997), ('padha', 119998)]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_2, _ = load_wiki(word_index, max_features, unk_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /kaggle/input/quora-insincere-questions-classification/embeddings.zip\r\n",
      "   creating: /kaggle/working/embeddings/paragram_300_sl999/\r\n",
      "  inflating: /kaggle/working/embeddings/paragram_300_sl999/README.txt  \r\n",
      "  inflating: /kaggle/working/embeddings/paragram_300_sl999/paragram_300_sl999.txt  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'paragram_300_sl999/*' -d /kaggle/working/embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unknowns parag 51355\n",
      "[('u50a', 119878), ('mineplex', 119895), ('pilapt', 119909), ('1bhks', 119948), ('technipfmc', 119949), ('spectrul', 119964), ('conjuct', 119976), ('quore', 119983), ('hurricains', 119994), ('padha', 119998)]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_3, _ = load_parag(word_index, max_features, unk_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /kaggle/input/quora-insincere-questions-classification/embeddings.zip\r\n",
      "   creating: /kaggle/working/embeddings/GoogleNews-vectors-negative300/\r\n",
      "  inflating: /kaggle/working/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip /kaggle/input/quora-insincere-questions-classification/embeddings.zip 'GoogleNews-vectors-negative300/*' -d /kaggle/working/embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unknowns ggle 77059\n",
      "[('whs', 119984), ('gopakumar', 119985), ('pittbull', 119986), ('4000m', 119987), ('bohuslän', 119990), ('cussler', 119992), ('hurricains', 119994), ('1090', 119996), ('roatan', 119997), ('padha', 119998)]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_4, _ = load_ggle(word_index, max_features, unk_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate((embedding_matrix_1,\n",
    "                                   embedding_matrix_2,\n",
    "                                   embedding_matrix_3,\n",
    "                                   embedding_matrix_4), axis=1)\n",
    "del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.Tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (proj): Linear(in_features=600, out_features=128, bias=True)\n",
      "  (proj_act): ReLU()\n",
      "  (gru): GRU(128, 128, batch_first=True, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 341121\n",
      "n_model:1 129.6s/epoch\n",
      "7.3s\n",
      "n_model:2 129.5s/epoch\n",
      "7.4s\n",
      "n_model:3 129.5s/epoch\n",
      "7.2s\n",
      "n_model:4 129.5s/epoch\n",
      "7.5s\n",
      "n_model:5 129.5s/epoch\n",
      "7.3s\n",
      "Done:6608.1s\n"
     ]
    }
   ],
   "source": [
    "ema_n = int(train_aug.shape[0] / (updates_per_epoch * batch_size))\n",
    "test_pr = np.zeros((len(val_df), 1))\n",
    "for i in range(n_models):\n",
    "    cols_in_use = cols_s[i % len(cols_s)]\n",
    "    model = GRUModel(n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim,\n",
    "                     pretrained_embedding=embedding_matrix[:, cols_in_use],\n",
    "                     fix_embedding=fix_embedding, padding_idx=0)\n",
    "    if i == 0:\n",
    "        print(model)\n",
    "        print('#Trainable params', get_param_size(model))\n",
    "    model.to(device)\n",
    "    ema_model = copy.deepcopy(model)\n",
    "    ema_model.eval()\n",
    "    optimizer = Adam(model.parameters())\n",
    "    ema = EMA(model, mu, n=ema_n)\n",
    "\n",
    "    t2 = time.time()\n",
    "    model.train()\n",
    "    model.fit(train_aug_loader, aug_epochs, optimizer, callbacks=[ema])\n",
    "    experiment.log_metric(\"aug model\", i + 1)\n",
    "    print(f'n_model:{i + 1} {(time.time() - t2) / (aug_epochs):.1f}s/epoch')\n",
    "\n",
    "    ema.set_weights(ema_model)\n",
    "    ema_model.gru.flatten_parameters()\n",
    "    t3 = time.time()\n",
    "    test_pr += ema_model.predict_proba(test_loader)\n",
    "    print(f'{time.time() - t3:.1f}s')\n",
    "test_pr /= n_models\n",
    "test_pr = (test_pr > threshold).astype(int)\n",
    "print(f'Done:{time.time() - t0:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation f1-score: 0.6931330888487883\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './GRUs-with-augmentation')\n",
    "f1 = f1_score(val_ans, test_pr)\n",
    "print('validation f1-score: {}'.format(f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
