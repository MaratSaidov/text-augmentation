{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.0)\r\n",
      "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.6.5)\r\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.22.0)\r\n",
      "Collecting subprocess32>=3.5.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 7.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.8.0)\r\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.1.2)\r\n",
      "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.352.0)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (1.13.0)\r\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (3.0.5)\r\n",
      "Collecting shortuuid>=0.5.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\r\n",
      "Collecting sentry-sdk>=0.4.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\r\n",
      "\u001b[K     |████████████████████████████████| 112kB 21.0MB/s \r\n",
      "\u001b[?25hCollecting gql==0.2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\r\n",
      "Collecting watchdog>=0.8.3\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 8.0MB/s \r\n",
      "\u001b[?25hCollecting configparser>=3.8.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\r\n",
      "Collecting docker-pycreds>=0.4.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (3.0.4)\r\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (1.24.2)\r\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (2019.9.11)\r\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0.0->wandb) (2.8)\r\n",
      "Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (2.0.6)\r\n",
      "Collecting graphql-core<2,>=0.5.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\r\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \r\n",
      "\u001b[?25hCollecting promise<3,>=2.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\r\n",
      "Collecting pathtools>=0.1.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\r\n",
      "Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\r\n",
      "Building wheels for collected packages: subprocess32, gql, watchdog, graphql-core, promise, pathtools\r\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=708de455b08e4f871e456e685272313158527c7fbf785a86d1c00eacc2dbb4b7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\r\n",
      "  Building wheel for gql (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=65e2270217ff8e98f8b4463ec7b037a6cd8b60f0db35f149c53122dcc706d289\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\r\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73604 sha256=5d61d43d8af3edc449cae0ceb5f0503257fc65af241c1d33fe13845a7713672a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\r\n",
      "  Building wheel for graphql-core (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104651 sha256=97f4b692f1f6490361b10ed222412b1c72e494d9203259a731a4b459da028227\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\r\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-cp36-none-any.whl size=21494 sha256=616a6424a9c4655282ae7580587eddb95322ec7c27fa4873b312c2e4dc40a214\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\r\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8786 sha256=87c5dc94cf2dfcde48ef964ad7cf4d549e65a628caff7fdf982261b575fe6000\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\r\n",
      "Successfully built subprocess32 gql watchdog graphql-core promise pathtools\r\n",
      "Installing collected packages: subprocess32, shortuuid, sentry-sdk, promise, graphql-core, gql, pathtools, watchdog, configparser, docker-pycreds, wandb\r\n",
      "Successfully installed configparser-5.0.0 docker-pycreds-0.4.0 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 promise-2.3 sentry-sdk-0.14.3 shortuuid-1.0.1 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login ee9416edde558c322450d0ec80266d2c0db81f45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/6c/ca85b6bd29926561229e8c9f677c36c65db9ef1947bfc175e6641bc82ace/nlpaug-0.0.14-py3-none-any.whl (101kB)\r\n",
      "\u001b[K     |████████████████████████████████| 102kB 2.4MB/s \r\n",
      "\u001b[?25hInstalling collected packages: nlpaug\r\n",
      "Successfully installed nlpaug-0.0.14\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting subword-nmt\r\n",
      "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\r\n",
      "Installing collected packages: subword-nmt\r\n",
      "Successfully installed subword-nmt-0.3.7\r\n"
     ]
    }
   ],
   "source": [
    "!pip install subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/augmented-fairseq/augmented (1).csv\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/augmented/augmented.csv\n",
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/paragram-300-sl999/paragram_300_sl999.txt\n",
      "/kaggle/input/bpe50k/bpe_codes.txt\n",
      "/kaggle/input/bpe50k/bpe_test.txt\n",
      "/kaggle/input/bpe50k/bpe_train.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/quora-insincere-questions-classification/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import scipy\n",
    "\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "data_path = '/kaggle/input/quora-insincere-questions-classification/'\n",
    "seed = 2077\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/working/wandb/settings\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_prob = 0.5\n",
    "# Preprocessing\n",
    "max_len = 50\n",
    "lower = True\n",
    "trunc = 'pre'\n",
    "max_features = 120000\n",
    "n_vocab = max_features\n",
    "clean_num = 0\n",
    "\n",
    "# Training\n",
    "aug_epochs = 5\n",
    "batch_size = 512\n",
    "drop_last = True\n",
    "dropout = 0.35\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "# Embedding\n",
    "fix_embedding = True\n",
    "unk_uni = True  # Initializer for unknown words делает вектор для незнакомого слова из N(emb.mean(), emb.std())\n",
    "n_embed = 2\n",
    "embed_dim = n_embed * 300 \n",
    "proj_dim = hidden_dim\n",
    "\n",
    "# GRU\n",
    "bidirectional = True\n",
    "n_layers = 1\n",
    "rnn_dim = hidden_dim\n",
    "\n",
    "# The second last Linear layer\n",
    "dense_dim = 2 * rnn_dim if bidirectional else rnn_dim\n",
    "\n",
    "# Test set\n",
    "test_batch_size = 8 * batch_size\n",
    "\n",
    "def seed_torch(seed=1):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_param_size(model, trainable=True):\n",
    "    if trainable:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n",
    "    else:\n",
    "        psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    return psize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z, _ = torch.max(inputs, 1)\n",
    "        return z\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim, dropout=0.2,\n",
    "                 padding_idx=0, pretrained_embedding=None, fix_embedding=True,\n",
    "                 n_out=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fix_embedding = fix_embedding\n",
    "        self.padding_idx = padding_idx\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(pretrained_embedding, freeze=fix_embedding)\n",
    "            self.embed.padding_idx = self.padding_idx\n",
    "        else:\n",
    "            self.embed = nn.Embedding(self.n_vocab, self.embed_dim, padding_idx=self.padding_idx)\n",
    "        self.embed_drop = nn.Dropout(dropout + 0.1)\n",
    "        self.linear_drop1 = nn.Dropout(dropout)\n",
    "        self.linear_drop2 = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_dim, rnn_dim, self.n_layers,\n",
    "                          batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.pooling = GlobalMaxPooling1D()\n",
    "        in_dim = 2 * rnn_dim if self.bidirectional else rnn_dim\n",
    "        self.dense = nn.Linear(in_dim, dense_dim)\n",
    "        self.dense_act = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(dense_dim)\n",
    "        self.out_linear = nn.Linear(dense_dim, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (bs, max_len)\n",
    "        x = self.embed_drop(self.embed(inputs))\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.linear_drop1(x)\n",
    "        x = self.dense_act(self.batchnorm(self.dense(x)))\n",
    "        x = self.linear_drop2(x)\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, dataloader, valloader, epochs, optimizer, scheduler, val_ans, patience=3, callbacks=None):\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "        for i in range(epochs):\n",
    "            print('epoch: ', i + 1)\n",
    "            self.run_epoch(dataloader, valloader, optimizer, scheduler, val_ans, early_stopping, callbacks=callbacks)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                X_batch = batch[0]\n",
    "                preds.append(self.forward(X_batch).data.cpu())\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def predict_proba(self, dataloader):\n",
    "        return torch.sigmoid(self.predict(dataloader)).data.numpy()\n",
    "    \n",
    "    def run_epoch(self, dataloader, valloader, optimizer, scheduler, val_ans, early_stopping, callbacks=None,\n",
    "              criterion=nn.BCEWithLogitsLoss()):\n",
    "        t1 = time.time()\n",
    "        tr_loss = 0\n",
    "        tr_roc_auc = 0\n",
    "        self.train()\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            x_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.forward(x_batch)\n",
    "            loss = criterion(outputs[:, 0], y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()\n",
    "            tr_roc_auc += roc_auc_score(y_batch.cpu(), torch.sigmoid(outputs).data.cpu().numpy())\n",
    "            if callbacks is not None:\n",
    "                for func in callbacks:\n",
    "                    func.on_batch_end(self)\n",
    "        tr_roc_auc /= step\n",
    "        tr_loss /= step\n",
    "        self.eval()\n",
    "        val_preds = self.predict(valloader)\n",
    "        val_ans = torch.tensor(val_ans.values, dtype=torch.float64)\n",
    "        val_loss = criterion(val_preds[:, 0], val_ans).item()\n",
    "        val_roc_auc = roc_auc_score(val_ans.int().cpu(), torch.sigmoid(val_preds).data.cpu().numpy())\n",
    "        print(f'val loss:{val_loss:.7f}')\n",
    "        print(f'train loss:{tr_loss:.7f}')\n",
    "        early_stopping(val_loss, self)    \n",
    "        if callbacks is not None:\n",
    "            for func in callbacks:\n",
    "                func.on_epoch_end(self)\n",
    "        wandb.log({\"loss/train\": tr_loss})\n",
    "        wandb.log({\"loss/val\": val_loss})\n",
    "        wandb.log({\"roc_auc/train\": tr_roc_auc})\n",
    "        wandb.log({\"roc_auc/val\": val_roc_auc})\n",
    "        scheduler.step(val_loss)\n",
    "        return tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    token_ids_s : List of token ids\n",
    "    labels   : Target labels\n",
    "    training: Sort token_ids_s by length if training is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_ids_s, labels=None, max_len=1000, training=True, sort=True):\n",
    "        self.training = training\n",
    "\n",
    "        if labels is None:\n",
    "            self.labels = torch.ones(len(token_ids_s), dtype=torch.long)  # dummy\n",
    "        else:\n",
    "            self.labels = torch.LongTensor(labels)\n",
    "\n",
    "        seq_lens = []\n",
    "        self.inputs = []\n",
    "        for e, token_ids in enumerate(token_ids_s):\n",
    "            seq_lens.append(len(token_ids))\n",
    "            input_ids = torch.LongTensor(token_ids[:max_len])\n",
    "            self.inputs.append(input_ids)\n",
    "\n",
    "        if self.training and sort:\n",
    "            self.indices = np.argsort(seq_lens)\n",
    "            self.inputs = [self.inputs[i] for i in self.indices]\n",
    "            self.labels = self.labels[self.indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Always drop last\n",
    "class BatchIterator(object):\n",
    "\n",
    "    def __init__(self, dataset, collate_fn, batch_size,\n",
    "                 shuffle=True, drop_last=True):\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(dataset)\n",
    "        self.shuffle = shuffle\n",
    "        if drop_last:\n",
    "            self.num_batches = self.size // batch_size\n",
    "        else:\n",
    "            self.num_batches = (self.size + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.random.choice(self.num_batches, self.num_batches, replace=False)\n",
    "        else:\n",
    "            indices = range(self.num_batches)\n",
    "        for idx in indices:\n",
    "            left = self.batch_size * idx\n",
    "            yield(self.collate_fn(self.dataset[left: left + self.batch_size]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xy_batch = [pad_sequence(batch[0], batch_first=True), batch[1]]\n",
    "    return xy_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = ',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║\\\n",
    "―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n",
    "\n",
    "\n",
    "def clean_text(x, puncts=puncts): #добавляет пробелы вокруг пунктуации\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(train_df, val_df, test_df, max_len, max_features, trunc='pre',\n",
    "                 lower=False, clean_num=2, unk_uni=True):\n",
    "    train_df = train_df.copy()\n",
    "    train_y = train_df['target'].values\n",
    "\n",
    "\n",
    "    # fill up the missing values\n",
    "    train_df['text'] = train_df['text'].fillna('_##_')\n",
    "    val_df['text'] = val_df['text'].fillna('_##_')\n",
    "    test_df['text'] = test_df['text'].fillna('_##_')\n",
    "\n",
    "    train_X = train_df['text'].values\n",
    "    val_X = val_df['text'].values\n",
    "    test_X = test_df['text'].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features, lower=True, filters='')\n",
    "    tokenizer.fit_on_texts(train_X)\n",
    "    print(tokenizer)\n",
    "    print('len(vocab)', len(tokenizer.word_index))\n",
    "\n",
    "    train_X_ids = tokenizer.texts_to_sequences(train_X)\n",
    "    train_dataset = TextClassificationDataset(train_X_ids, train_y, sort=False)\n",
    "    train_loader = BatchIterator(train_dataset, collate_fn, batch_size)\n",
    "    \n",
    "    val_X_ids = tokenizer.texts_to_sequences(val_X)\n",
    "    val_dataset = TextClassificationDataset(val_X_ids, training=False)\n",
    "    val_loader = BatchIterator(val_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "    test_X_ids = tokenizer.texts_to_sequences(test_X)\n",
    "    test_dataset = TextClassificationDataset(test_X_ids, training=False)\n",
    "    test_loader = BatchIterator(test_dataset, collate_fn, batch_size, shuffle=False, drop_last=False)\n",
    "    return train_loader, val_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path  + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmented(p=0.35):\n",
    "    aug = nac.KeyboardAug(aug_word_p=p)\n",
    "    augmented_list = aug.augments(oneline.iloc[o_train_idx]['text'].values)\n",
    "    augmented = pd.DataFrame({'text' : augmented_list, \n",
    "                          'target' : list(oneline.iloc[o_train_idx]['target'].values)})\n",
    "    return augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>487207</th>\n",
       "      <td>5f6988e5cb1096691086</td>\n",
       "      <td>when will men and boys learn the truth and go ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113645</th>\n",
       "      <td>da36c0a619e1cf3fcdd4</td>\n",
       "      <td>why does not keralites  ( india )  does not li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558619</th>\n",
       "      <td>6d73fed2c64a4883e27b</td>\n",
       "      <td>will you still incur a cash advance fee if you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807057</th>\n",
       "      <td>9e22ba5c0a04d9a44236</td>\n",
       "      <td>what are the real advantages of using quora ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163578</th>\n",
       "      <td>e4006d45becca4c20ab7</td>\n",
       "      <td>how much coffee is safe to drink ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "487207   5f6988e5cb1096691086   \n",
       "1113645  da36c0a619e1cf3fcdd4   \n",
       "558619   6d73fed2c64a4883e27b   \n",
       "807057   9e22ba5c0a04d9a44236   \n",
       "1163578  e4006d45becca4c20ab7   \n",
       "\n",
       "                                                      text  target  \n",
       "487207   when will men and boys learn the truth and go ...       1  \n",
       "1113645  why does not keralites  ( india )  does not li...       0  \n",
       "558619   will you still incur a cash advance fee if you...       0  \n",
       "807057      what are the real advantages of using quora ?        0  \n",
       "1163578                how much coffee is safe to drink ?        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.rename(columns={'question_text': 'text'})\n",
    "\n",
    "data['text'] = data['text'].apply(str.lower)\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "data['text'] = data['text'].apply(clean_numbers)\n",
    "\n",
    "train, test= train_test_split(data.copy(), train_size=0.7, random_state=seed)\n",
    "\n",
    "display(train.head())\n",
    "\n",
    "oneline = train[~train['text'].str.contains('\\n')]\n",
    "index = np.arange(len(train.index))\n",
    "multiline_idx = index[list(train['text'].str.contains('\\n'))]\n",
    "oneline_idx = index[list(~train['text'].str.contains('\\n'))]\n",
    "\n",
    "train['text'] = train['text'].str.replace('\\n', '\\t')\n",
    "test['text'] = test['text'].str.replace('\\n', '\\t')\n",
    "\n",
    "oneline_size = len(oneline)\n",
    "o_train_idx, o_valid_idx = train_test_split(np.arange(oneline_size), train_size=0.9, random_state=seed)\n",
    "\n",
    "train_idx = oneline_idx[o_train_idx]\n",
    "valid_idx = oneline_idx[o_valid_idx]\n",
    "train_idx = np.hstack((train_idx, multiline_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bpe = pd.read_csv('/kaggle/input/bpe50k/bpe_train.txt', sep='\\n', header=None)\n",
    "test_bpe = pd.read_csv('/kaggle/input/bpe50k/bpe_test.txt', sep='\\n', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.DataFrame({\n",
    "    'text' :\n",
    "    list(train_bpe.loc[valid_idx][0].values),\n",
    "    'target':\n",
    "    list(train.iloc[valid_idx]['target'].values) \n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text' :\n",
    "    list(test_bpe[0].values),\n",
    "    'target':\n",
    "    list(test['target'].values) \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"subword-nmt apply-bpe -c /kaggle/input/bpe50k/bpe_codes.txt < /kaggle/working/train.txt > check.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(aug_prob, epochs):\n",
    "    wandb.init(project=\"trained embeddings bpe\", name=f'bpe keyboard {aug_prob}',reinit=True)\n",
    "    #wandb.init(project=\"single model\", name=f'bpe keyboard {aug_prob}',reinit=True)\n",
    "    \n",
    "    augmented = make_augmented(aug_prob)\n",
    "    \n",
    "    augmented['text'] = augmented['text'].apply(str.lower)\n",
    "    augmented['text'] = augmented['text'].apply(clean_text)\n",
    "    augmented['text'] = augmented['text'].apply(clean_numbers)\n",
    "    \n",
    "    np.savetxt(r'./aug.txt', augmented['text'].values, fmt='%s')\n",
    "    os.system(\"subword-nmt apply-bpe -c /kaggle/input/bpe50k/bpe_codes.txt < /kaggle/working/aug.txt > bpe_aug.txt\")\n",
    "    aug_bpe = pd.read_csv('/kaggle/working/bpe_aug.txt', sep='\\n', header=None)\n",
    "    \n",
    "    train_aug = pd.DataFrame({\n",
    "        'text' : \n",
    "        list(train_bpe.loc[train_idx][0].values) +\n",
    "        list(aug_bpe[0].values),\n",
    "        'target' :\n",
    "        list(train.iloc[train_idx]['target'].values) +\n",
    "        list(train.iloc[oneline_idx[o_train_idx]]['target'].values)\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    train_aug.drop_duplicates(inplace=True)\n",
    "    train_aug = train_aug.sample(frac=1)\n",
    "    \n",
    "    #del augmented\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    data_path = '/kaggle/working/'\n",
    "    obj = prepare_data(train_aug, valid_df, test_df, max_len, max_features,\n",
    "                   trunc=trunc, lower=lower, clean_num=clean_num, unk_uni=unk_uni)\n",
    "\n",
    "    train_loader, val_loader, test_loader, tokenizer = obj\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    val_ans, test_ans = valid_df['target'], test_df['target']\n",
    "\n",
    "    model = GRUModel(n_vocab, embed_dim, proj_dim, rnn_dim, n_layers, bidirectional, dense_dim, padding_idx=0)\n",
    "    print(model)\n",
    "    print('#Trainable params', get_param_size(model))\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.0005)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=2, factor=0.5)\n",
    "    \n",
    "    config =  {\n",
    "            'model': 'bigru',\n",
    "            'optimizer': str(optimizer),\n",
    "            'lr' : 0.0005,\n",
    "            'scheduler': 'plateau',\n",
    "            'early_stop': 3,\n",
    "            'vocab_size': n_vocab,\n",
    "             'h_size': hidden_dim,\n",
    "             'n_layers': n_layers,\n",
    "             'dropout': dropout,\n",
    "             'batch_size': batch_size,\n",
    "             'embed_dim': embed_dim,\n",
    "             'max_len': max_len,\n",
    "             'batch_norm' : 'true'\n",
    "        }\n",
    "    wandb.config.update(config)\n",
    "    \n",
    "    val_probas = np.zeros((len(valid_df), 1))\n",
    "    test_probas = np.zeros((len(test_df), 1))\n",
    "\n",
    "    t2 = time.time()\n",
    "    wandb.watch(model)\n",
    "    model.fit(train_loader, val_loader, epochs, optimizer, scheduler, val_ans)\n",
    "    print(f'n_model:{1} {(time.time() - t2) / (aug_epochs):.1f}s/epoch')\n",
    "    t3 = time.time()\n",
    "    val_probas =  model.predict_proba(val_loader)\n",
    "    test_probas = model.predict_proba(test_loader)\n",
    "    print(f'n_model:{1} {roc_auc_score(val_ans, val_probas)}')\n",
    "    print(f'{time.time() - t3:.1f}s')\n",
    "\n",
    "    res = scipy.optimize.minimize(\n",
    "        lambda t: -f1_score(val_ans, (val_probas >= t).astype(np.int)),\n",
    "        x0=0.5,\n",
    "        method='Nelder-Mead',\n",
    "        tol=1e-3,\n",
    "    )\n",
    "    best_threshold = res.x[0]\n",
    "    print(best_threshold)\n",
    "    \n",
    "    f1_val = f1_score(val_ans, (val_probas >= best_threshold))\n",
    "    f1_test = f1_score(test_ans, test_probas >= best_threshold)\n",
    "    wandb.log({'f1/val' : f1_val})\n",
    "    wandb.log({'f1/test' : f1_test})\n",
    "    roc_auc = roc_auc_score(test_ans, test_probas)\n",
    "    print('test f1-score: {}'.format(f1_test))\n",
    "    print('test roc-auc score: {}'.format(roc_auc))\n",
    "    \n",
    "    wandb.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/38gopaxe\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/38gopaxe</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7efc542d5fd0>\n",
      "len(vocab) 51344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1204677\n",
      "train loss:0.1489946\n",
      "Validation loss decreased (inf --> 0.120468).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1116152\n",
      "train loss:0.1188688\n",
      "Validation loss decreased (0.120468 --> 0.111615).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1106562\n",
      "train loss:0.1092127\n",
      "Validation loss decreased (0.111615 --> 0.110656).  Saving model ...\n",
      "epoch:  4\n",
      "val loss:0.1101009\n",
      "train loss:0.1025658\n",
      "Validation loss decreased (0.110656 --> 0.110101).  Saving model ...\n",
      "epoch:  5\n",
      "val loss:0.1102031\n",
      "train loss:0.0964527\n",
      "EarlyStopping counter: 1 out of 3\n",
      "n_model:1 120.9s/epoch\n",
      "n_model:1 0.9573489951581379\n",
      "10.5s\n",
      "0.33437499999999987\n",
      "test f1-score: 0.6476858671214824\n",
      "test roc-auc score: 0.9566678525110016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/1i6k8fbq\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/1i6k8fbq</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7efc8ef8b7f0>\n",
      "len(vocab) 51463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1184834\n",
      "train loss:0.1541335\n",
      "Validation loss decreased (inf --> 0.118483).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1117347\n",
      "train loss:0.1201523\n",
      "Validation loss decreased (0.118483 --> 0.111735).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1091900\n",
      "train loss:0.1105472\n",
      "Validation loss decreased (0.111735 --> 0.109190).  Saving model ...\n",
      "n_model:1 82.9s/epoch\n",
      "n_model:1 0.9581256511274441\n",
      "11.0s\n",
      "0.35781249999999987\n",
      "test f1-score: 0.6466174298375184\n",
      "test roc-auc score: 0.9576605349955984\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/3mbs9xsk\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/3mbs9xsk</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7efc53a9b630>\n",
      "len(vocab) 51531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1184697\n",
      "train loss:0.1507796\n",
      "Validation loss decreased (inf --> 0.118470).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1105721\n",
      "train loss:0.1196712\n",
      "Validation loss decreased (0.118470 --> 0.110572).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1087281\n",
      "train loss:0.1105706\n",
      "Validation loss decreased (0.110572 --> 0.108728).  Saving model ...\n",
      "epoch:  4\n",
      "val loss:0.1092051\n",
      "train loss:0.1038965\n",
      "EarlyStopping counter: 1 out of 3\n",
      "n_model:1 117.5s/epoch\n",
      "n_model:1 0.9587758759035444\n",
      "11.1s\n",
      "0.2617187499999998\n",
      "test f1-score: 0.6482662076010669\n",
      "test roc-auc score: 0.9577966538913223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/kg84fygj\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/kg84fygj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7efc4b5d09e8>\n",
      "len(vocab) 51607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1179618\n",
      "train loss:0.1557921\n",
      "Validation loss decreased (inf --> 0.117962).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1113728\n",
      "train loss:0.1219522\n",
      "Validation loss decreased (0.117962 --> 0.111373).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1092255\n",
      "train loss:0.1129113\n",
      "Validation loss decreased (0.111373 --> 0.109226).  Saving model ...\n",
      "epoch:  4\n",
      "val loss:0.1094877\n",
      "train loss:0.1062305\n",
      "EarlyStopping counter: 1 out of 3\n",
      "n_model:1 120.6s/epoch\n",
      "n_model:1 0.9584242150355201\n",
      "10.9s\n",
      "0.29374999999999984\n",
      "test f1-score: 0.6491007298704818\n",
      "test roc-auc score: 0.9579974569593909\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/1uqgs373\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/1uqgs373</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7efc4d7def98>\n",
      "len(vocab) 51640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1181775\n",
      "train loss:0.1526527\n",
      "Validation loss decreased (inf --> 0.118178).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1112357\n",
      "train loss:0.1222931\n",
      "Validation loss decreased (0.118178 --> 0.111236).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1089275\n",
      "train loss:0.1133895\n",
      "Validation loss decreased (0.111236 --> 0.108927).  Saving model ...\n",
      "epoch:  4\n",
      "val loss:0.1084509\n",
      "train loss:0.1070571\n",
      "Validation loss decreased (0.108927 --> 0.108451).  Saving model ...\n",
      "epoch:  5\n",
      "val loss:0.1094411\n",
      "train loss:0.1017137\n",
      "EarlyStopping counter: 1 out of 3\n",
      "n_model:1 152.0s/epoch\n",
      "n_model:1 0.958983956691176\n",
      "11.0s\n",
      "0.2624999999999998\n",
      "test f1-score: 0.6485664965475832\n",
      "test roc-auc score: 0.9582624547195504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/3f8aztcc\" target=\"_blank\">https://app.wandb.ai/arinaruck/trained%20embeddings%20bpe/runs/3f8aztcc</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7efc53e60be0>\n",
      "len(vocab) 51665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (embed): Embedding(120000, 600, padding_idx=0)\n",
      "  (embed_drop): Dropout(p=0.30000000000000004, inplace=False)\n",
      "  (linear_drop1): Dropout(p=0.2, inplace=False)\n",
      "  (linear_drop2): Dropout(p=0.2, inplace=False)\n",
      "  (gru): GRU(600, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (pooling): GlobalMaxPooling1D()\n",
      "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dense_act): ReLU()\n",
      "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "#Trainable params 72627201\n",
      "epoch:  1\n",
      "val loss:0.1192740\n",
      "train loss:0.1518616\n",
      "Validation loss decreased (inf --> 0.119274).  Saving model ...\n",
      "epoch:  2\n",
      "val loss:0.1120746\n",
      "train loss:0.1237092\n",
      "Validation loss decreased (0.119274 --> 0.112075).  Saving model ...\n",
      "epoch:  3\n",
      "val loss:0.1096203\n",
      "train loss:0.1150295\n",
      "Validation loss decreased (0.112075 --> 0.109620).  Saving model ...\n",
      "n_model:1 92.0s/epoch\n",
      "n_model:1 0.9569009432662624\n",
      "10.8s\n",
      "0.32812499999999983\n",
      "test f1-score: 0.6441607238708689\n",
      "test roc-auc score: 0.9567869562697237\n"
     ]
    }
   ],
   "source": [
    "augs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "epochs_num = [5, 3, 4, 4, 5, 3]\n",
    "\n",
    "for aug_p, epochs in zip(augs, epochs_num):\n",
    "    train_model(aug_p, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
